{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea20874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import MultiTaskDataset\n",
    "import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78e04ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.load('inputs.npy').astype(np.float32)\n",
    "output_data = np.load('outputs.npy').astype(np.float32)\n",
    "\n",
    "train_int, dev_int, train_out, dev_out = train_test_split(input_data, output_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_out1 = train_out[:, 0, :]\n",
    "train_out2 = train_out[:, 1, :]\n",
    "train_out3 = train_out[:, 2, :]\n",
    "\n",
    "dev_out1 = dev_out[:, 0, :]\n",
    "dev_out2 = dev_out[:, 1, :]\n",
    "dev_out3 = dev_out[:, 2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "418b7cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "920.2192 -0.67927146 741954.56\n",
      "0.0086741885 -0.3333267 0.6572305\n",
      "0.004157325 -0.3333333 0.66662663\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_out1), np.min(train_out1), np.max(train_out1))\n",
    "print(np.mean(train_out2), np.min(train_out2), np.max(train_out2))\n",
    "print(np.mean(train_out3), np.min(train_out3), np.max(train_out3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6405a3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm1 = np.load('Bolplanck_DM_xi.npy').astype(np.float32)\n",
    "train_out1_dm = train_out1 / dm1\n",
    "dev_out1_dm = dev_out1 / dm1\n",
    "\n",
    "train_out2_dm_og = train_out2 / dm1\n",
    "dev_out2_dm_og = dev_out2 / dm1\n",
    "\n",
    "train_out3_dm_og = train_out3 / dm1\n",
    "dev_out3_dm_og = dev_out3 / dm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4acf3aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8700614 -1.815475 1162.3429\n",
      "0.0017585435 -1.5572307 1.1663933\n",
      "0.0003111336 -1.6395723 1.720901\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_out1_dm), np.min(train_out1_dm), np.max(train_out1_dm))\n",
    "print(np.mean(train_out2_dm_og), np.min(train_out2_dm_og), np.max(train_out2_dm_og))\n",
    "print(np.mean(train_out3_dm_og), np.min(train_out3_dm_og), np.max(train_out3_dm_og))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "293d6469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(502,)\n"
     ]
    }
   ],
   "source": [
    "indices = np.any(dev_out1_dm > 100, axis=1)\n",
    "rows_to_delete_dev = np.where(indices)[0]\n",
    "print(rows_to_delete_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4a7985e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1881,)\n"
     ]
    }
   ],
   "source": [
    "indices = np.any(train_out1_dm > 100, axis=1)\n",
    "rows_to_delete_train = np.where(indices)[0]\n",
    "print(rows_to_delete_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a549237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out1_dm = np.delete(train_out1_dm, rows_to_delete_train, axis=0)\n",
    "dev_out1_dm = np.delete(dev_out1_dm, rows_to_delete_dev, axis=0)\n",
    "\n",
    "train_out2_dm_og = np.delete(train_out2_dm_og, rows_to_delete_train, axis=0)\n",
    "dev_out2_dm_og = np.delete(dev_out2_dm_og, rows_to_delete_dev, axis=0)\n",
    "\n",
    "train_out3_dm_og = np.delete(train_out3_dm_og, rows_to_delete_train, axis=0)\n",
    "dev_out3_dm_og = np.delete(dev_out3_dm_og, rows_to_delete_dev, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b31f554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0426817 -0.019937249 99.98336\n",
      "0.0016855038 -0.55319476 0.69765794\n",
      "0.0002882932 -0.54974824 0.64470935\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_out1_dm), np.min(train_out1_dm), np.max(train_out1_dm))\n",
    "print(np.mean(train_out2_dm_og), np.min(train_out2_dm_og), np.max(train_out2_dm_og))\n",
    "print(np.mean(train_out3_dm_og), np.min(train_out3_dm_og), np.max(train_out3_dm_og))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c19a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_wise_r(A, B):\n",
    "    r = np.zeros(A.shape[1])\n",
    "    for col in range(A.shape[1]):\n",
    "        r[col] = np.mean(abs(A[:, col] / B[:, col]))\n",
    "        r[col] = np.sqrt(r[col])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6060b1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[897.1982   673.00494  679.99384  562.6633   420.83118  361.24155\n",
      " 349.27252  317.8694   194.38054  127.01312   90.09065   89.07621\n",
      "  50.156803  50.599228  49.930668  45.284874  45.746452  41.250507\n",
      "  32.769413  27.539112]\n",
      "[1074.1969   1043.1628    989.0476   1076.9342    840.737     807.39703\n",
      " 1111.9567    998.92377   988.8078    540.1072    461.9973    373.55136\n",
      "  232.99428   146.7894    202.56018   181.88087   145.0774    176.11647\n",
      "  128.44656   113.359924]\n"
     ]
    }
   ],
   "source": [
    "r12 = column_wise_r(train_out1_dm, train_out2_dm_og).astype(np.float32)\n",
    "print(r12)\n",
    "r13 = column_wise_r(train_out1_dm, train_out3_dm_og).astype(np.float32)\n",
    "print(r13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4fe9c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out2_dm = train_out2_dm_og * r12 * 5\n",
    "train_out3_dm = train_out3_dm_og * r13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3755964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.454422 -86.09015 96.06441\n",
      "0.06653836 -86.251595 96.081215\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_out2_dm), np.min(train_out2_dm), np.max(train_out2_dm))\n",
    "print(np.mean(train_out3_dm), np.min(train_out3_dm), np.max(train_out3_dm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4ac4d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_out2_dm = dev_out2_dm_og * r12 * 5\n",
    "dev_out3_dm = dev_out3_dm_og * r13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b466148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43712196 -66.888336 113.63487\n",
      "0.06922737 -101.95363 74.44943\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(dev_out2_dm), np.min(dev_out2_dm), np.max(dev_out2_dm))\n",
    "print(np.mean(dev_out3_dm), np.min(dev_out3_dm), np.max(dev_out3_dm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59cd34b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1366128 463537 129577\n"
     ]
    }
   ],
   "source": [
    "count1 = np.count_nonzero(train_out1_dm > 1)\n",
    "count2 = np.count_nonzero(train_out2_dm > 1)\n",
    "count3 = np.count_nonzero(train_out3_dm > 1)\n",
    "print(count1, count2, count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e89a2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "count1 = np.count_nonzero(train_out1_dm < 0)\n",
    "print(count1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d771f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163722 40659 9031\n"
     ]
    }
   ],
   "source": [
    "count1 = np.count_nonzero(train_out1_dm > 10)\n",
    "count2 = np.count_nonzero(train_out2_dm > 10)\n",
    "count3 = np.count_nonzero(train_out3_dm > 10)\n",
    "print(count1, count2, count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a7a72a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364 3 1\n"
     ]
    }
   ],
   "source": [
    "count1 = np.count_nonzero(train_out1_dm > 90)\n",
    "count2 = np.count_nonzero(train_out2_dm > 90)\n",
    "count3 = np.count_nonzero(train_out3_dm > 90)\n",
    "print(count1, count2, count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bea61dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_int = np.delete(train_int, rows_to_delete_train, axis=0)\n",
    "dev_int = np.delete(dev_int, rows_to_delete_dev, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f33831f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultiTaskDataset.MultiTaskDataset(train_int, train_out1_dm, train_out2_dm, train_out3_dm)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "dev_dataset = MultiTaskDataset.MultiTaskDataset(dev_int, dev_out1_dm, dev_out2_dm, dev_out3_dm)\n",
    "dev_dataloader = DataLoader(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d512980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCED(nn.Module):\n",
    "    def __init__(self, input_dim=7, hidden_dim=128, bottleneck_dim=64, output_dim=20):\n",
    "        super(FCED, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.BatchNorm1d(hidden_dim),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.BatchNorm1d(hidden_dim),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.BatchNorm1d(hidden_dim),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.BatchNorm1d(hidden_dim),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.BatchNorm1d(hidden_dim),\n",
    "#             nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, bottleneck_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(bottleneck_dim, bottleneck_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(bottleneck_dim, bottleneck_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(bottleneck_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=bottleneck_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=bottleneck_dim, out_channels=bottleneck_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "#             nn.Conv1d(in_channels=bottleneck_dim, out_channels=bottleneck_dim, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Conv1d(in_channels=bottleneck_dim, out_channels=bottleneck_dim, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=bottleneck_dim, out_channels=output_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(output_dim*hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=bottleneck_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=bottleneck_dim, out_channels=bottleneck_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=bottleneck_dim, out_channels=bottleneck_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "#             nn.Conv1d(in_channels=bottleneck_dim, out_channels=bottleneck_dim, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=bottleneck_dim, out_channels=output_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(output_dim*hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        task1 = self.fc(encoded)\n",
    "        \n",
    "        encoded = encoded.view(encoded.size(0), 1, -1)\n",
    "        \n",
    "        task2 = self.decoder1(encoded)\n",
    "        task3 = self.decoder2(encoded)\n",
    "        \n",
    "        outputs = torch.stack((task1, task2, task3), dim=1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9c90747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_name, train_loader, dev_loader, num_epochs=2000, learning_rate=0.05, weight_decay=1e-4):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=np.arange(100, 1501, 100), gamma=0.5)\n",
    "    \n",
    "    best_model = None\n",
    "    best_aapd = None\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_vals = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            input_data = batch['input_data']\n",
    "            target1 = batch['out1']\n",
    "            target2 = batch['out2']\n",
    "            target3 = batch['out3']\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_data)\n",
    "            #print(outputs.shape)\n",
    "            train_loss1 = criterion(outputs[:, 0, :], target1)\n",
    "            train_loss2 = criterion(outputs[:, 1, :], target2)\n",
    "            train_loss3 = criterion(outputs[:, 2, :], target3)\n",
    "            \n",
    "            train_loss = train_loss1 + train_loss2 + train_loss3\n",
    "            \n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_losses.append(train_loss.item())\n",
    "        \n",
    "#         model.eval()\n",
    "        \n",
    "#         total_aapd = 0\n",
    "#         num_samples = 0\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for batch in dev_loader:\n",
    "#                 dev_int = batch['input_data']\n",
    "#                 dev_out1 = batch['out1']\n",
    "#                 dev_out2 = batch['out2']\n",
    "#                 dev_out3 = batch['out3']\n",
    "                \n",
    "#                 predictions = model(dev_int)\n",
    "                \n",
    "#                 val_loss1 = criterion(predictions[:, 0, :], dev_out1)\n",
    "#                 batch_aapd1 = AAPD(predictions[:, 0, :], dev_out1)\n",
    "                \n",
    "#                 val_loss2 = criterion(predictions[:, 1, :], dev_out2)\n",
    "#                 batch_aapd2 = AAPD(predictions[:, 1, :], dev_out2)\n",
    "                \n",
    "#                 val_loss3 = criterion(predictions[:, 2, :], dev_out3)\n",
    "#                 batch_aapd3 = AAPD(predictions[:, 2, :], dev_out3)\n",
    "                \n",
    "#                 val_loss = val_loss1 + val_loss2 + val_loss3\n",
    "#                 batch_aapd = (batch_aapd1 + batch_aapd2 + batch_aapd3) / 3\n",
    "                    \n",
    "#                 total_aapd += batch_aapd * len(dev_int)\n",
    "#                 num_samples += len(dev_int)\n",
    "            \n",
    "#             val_losses.append(val_loss.item())\n",
    "#             aapd = total_aapd / num_samples\n",
    "#             val_vals.append(aapd)\n",
    "            \n",
    "#             if best_aapd == None or aapd < best_aapd:\n",
    "#                 best_aapd = aapd\n",
    "#                 best_model = model\n",
    "#                 torch.save(model.state_dict(), model_name)\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {train_loss.item():.4f}')#; AAPD: {aapd}')\n",
    "        \n",
    "    plt.plot(np.arange(num_epochs), train_losses, '.', label = 'train set')\n",
    "    #plt.plot(np.arange(num_epochs), val_losses, 'x', label = 'val set')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('num epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    #plt.save('loss.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "#     plt.plot(np.arange(len(val_vals)), val_vals)\n",
    "#     plt.xlabel('num epochs')\n",
    "#     plt.ylabel('<|$\\hat{y}$ - y| / y>')\n",
    "#     plt.xscale('log')\n",
    "#     plt.yscale('log')\n",
    "#     #plt.save('validation.pdf')\n",
    "#     plt.show()\n",
    "\n",
    "#    return best_model, train_losses, val_losses, val_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a5f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000] - Loss: 4382.5576\n",
      "Epoch [2/2000] - Loss: 119.3752\n",
      "Epoch [3/2000] - Loss: 71.9488\n",
      "Epoch [4/2000] - Loss: 242.1225\n",
      "Epoch [5/2000] - Loss: 291.6582\n",
      "Epoch [6/2000] - Loss: 59.7375\n",
      "Epoch [7/2000] - Loss: 137.5274\n",
      "Epoch [8/2000] - Loss: 428.1820\n",
      "Epoch [9/2000] - Loss: 3711711.0000\n",
      "Epoch [10/2000] - Loss: 181368624.0000\n",
      "Epoch [11/2000] - Loss: 4426292.5000\n",
      "Epoch [12/2000] - Loss: 33116016.0000\n",
      "Epoch [13/2000] - Loss: 39356732.0000\n",
      "Epoch [14/2000] - Loss: 4760542.5000\n",
      "Epoch [15/2000] - Loss: 10593319.0000\n",
      "Epoch [16/2000] - Loss: 422877.1250\n",
      "Epoch [17/2000] - Loss: 7506933.5000\n",
      "Epoch [18/2000] - Loss: 10832909.0000\n",
      "Epoch [19/2000] - Loss: 4932703.0000\n",
      "Epoch [20/2000] - Loss: 15306989.0000\n",
      "Epoch [21/2000] - Loss: 473676.1562\n",
      "Epoch [22/2000] - Loss: 22757346.0000\n",
      "Epoch [23/2000] - Loss: 36883.3281\n",
      "Epoch [24/2000] - Loss: 229739.0781\n",
      "Epoch [25/2000] - Loss: 821671.7500\n",
      "Epoch [26/2000] - Loss: 719777.8750\n",
      "Epoch [27/2000] - Loss: 500510.4062\n",
      "Epoch [28/2000] - Loss: 153968.7188\n",
      "Epoch [29/2000] - Loss: 58878.6328\n",
      "Epoch [30/2000] - Loss: 1002390.7500\n",
      "Epoch [31/2000] - Loss: 715501.2500\n",
      "Epoch [32/2000] - Loss: 20283322.0000\n",
      "Epoch [33/2000] - Loss: 40246160.0000\n",
      "Epoch [34/2000] - Loss: 2536435.0000\n",
      "Epoch [35/2000] - Loss: 32861.6680\n",
      "Epoch [36/2000] - Loss: 8825777.0000\n",
      "Epoch [37/2000] - Loss: 255583.0156\n",
      "Epoch [38/2000] - Loss: 26446.5527\n",
      "Epoch [39/2000] - Loss: 64077.9922\n",
      "Epoch [40/2000] - Loss: 75924.6250\n",
      "Epoch [41/2000] - Loss: 9958.4531\n",
      "Epoch [42/2000] - Loss: 60285.0352\n",
      "Epoch [43/2000] - Loss: 15619.4473\n",
      "Epoch [44/2000] - Loss: 60637.4727\n",
      "Epoch [45/2000] - Loss: 86149.9844\n",
      "Epoch [46/2000] - Loss: 571658.0000\n",
      "Epoch [47/2000] - Loss: 3431.2292\n",
      "Epoch [48/2000] - Loss: 77895.6484\n",
      "Epoch [49/2000] - Loss: 40004.4062\n",
      "Epoch [50/2000] - Loss: 193292.4688\n",
      "Epoch [51/2000] - Loss: 33244.4492\n",
      "Epoch [52/2000] - Loss: 20844.7559\n",
      "Epoch [53/2000] - Loss: 10298.5039\n",
      "Epoch [54/2000] - Loss: 7518.4248\n",
      "Epoch [55/2000] - Loss: 9969.3408\n",
      "Epoch [56/2000] - Loss: 883207.0625\n",
      "Epoch [57/2000] - Loss: 15999.6533\n",
      "Epoch [58/2000] - Loss: 80114.5938\n",
      "Epoch [59/2000] - Loss: 52934.6680\n",
      "Epoch [60/2000] - Loss: 256634.9062\n",
      "Epoch [61/2000] - Loss: 11944.4316\n",
      "Epoch [62/2000] - Loss: 4875.0132\n",
      "Epoch [63/2000] - Loss: 3381.7607\n",
      "Epoch [64/2000] - Loss: 2073.3391\n",
      "Epoch [65/2000] - Loss: 5599.0747\n",
      "Epoch [66/2000] - Loss: 3545.2246\n",
      "Epoch [67/2000] - Loss: 1435.5463\n",
      "Epoch [68/2000] - Loss: 20305.1914\n",
      "Epoch [69/2000] - Loss: 44523.7461\n",
      "Epoch [70/2000] - Loss: 10633.8975\n",
      "Epoch [71/2000] - Loss: 180963.7656\n",
      "Epoch [72/2000] - Loss: 17345398.0000\n",
      "Epoch [73/2000] - Loss: 142612.1406\n",
      "Epoch [74/2000] - Loss: 3369.3069\n",
      "Epoch [75/2000] - Loss: 33631.6172\n",
      "Epoch [76/2000] - Loss: 1089393.3750\n",
      "Epoch [77/2000] - Loss: 873.4228\n",
      "Epoch [78/2000] - Loss: 4837.2896\n",
      "Epoch [79/2000] - Loss: 1584.4629\n",
      "Epoch [80/2000] - Loss: 1049.5258\n",
      "Epoch [81/2000] - Loss: 839.9299\n",
      "Epoch [82/2000] - Loss: 3792.5996\n",
      "Epoch [83/2000] - Loss: 362.9986\n",
      "Epoch [84/2000] - Loss: 6311.3062\n",
      "Epoch [85/2000] - Loss: 10999.8164\n",
      "Epoch [86/2000] - Loss: 38424.6172\n",
      "Epoch [87/2000] - Loss: 3777.0693\n",
      "Epoch [88/2000] - Loss: 643518.1250\n",
      "Epoch [89/2000] - Loss: 232483.9062\n",
      "Epoch [90/2000] - Loss: 159201.7188\n",
      "Epoch [91/2000] - Loss: 246470.3594\n",
      "Epoch [92/2000] - Loss: 188701.3125\n",
      "Epoch [93/2000] - Loss: 3013794.5000\n",
      "Epoch [94/2000] - Loss: 1646039.3750\n",
      "Epoch [95/2000] - Loss: 3521011.5000\n",
      "Epoch [96/2000] - Loss: 1687900.1250\n",
      "Epoch [97/2000] - Loss: 408741.3750\n",
      "Epoch [98/2000] - Loss: 1803365.0000\n",
      "Epoch [99/2000] - Loss: 209969.8906\n",
      "Epoch [100/2000] - Loss: 578887.7500\n",
      "Epoch [101/2000] - Loss: 186662.7188\n",
      "Epoch [102/2000] - Loss: 162222.9688\n",
      "Epoch [103/2000] - Loss: 155145.7812\n",
      "Epoch [104/2000] - Loss: 269661.5000\n",
      "Epoch [105/2000] - Loss: 103918.7891\n",
      "Epoch [106/2000] - Loss: 2920152.0000\n",
      "Epoch [107/2000] - Loss: 2273447.2500\n",
      "Epoch [108/2000] - Loss: 2537449.2500\n",
      "Epoch [109/2000] - Loss: 663983.8125\n",
      "Epoch [110/2000] - Loss: 182583.1719\n",
      "Epoch [111/2000] - Loss: 976382.8125\n",
      "Epoch [112/2000] - Loss: 40654.2773\n",
      "Epoch [113/2000] - Loss: 70940.3828\n",
      "Epoch [114/2000] - Loss: 30311.7051\n",
      "Epoch [115/2000] - Loss: 54826.9688\n",
      "Epoch [116/2000] - Loss: 19395.2793\n",
      "Epoch [117/2000] - Loss: 6026.8511\n",
      "Epoch [118/2000] - Loss: 19370.5566\n",
      "Epoch [119/2000] - Loss: 44111.7891\n",
      "Epoch [120/2000] - Loss: 2557946.0000\n",
      "Epoch [121/2000] - Loss: 140186.5000\n",
      "Epoch [122/2000] - Loss: 405145.5938\n",
      "Epoch [123/2000] - Loss: 310281.3750\n",
      "Epoch [124/2000] - Loss: 813869.0625\n",
      "Epoch [125/2000] - Loss: 28514.2402\n",
      "Epoch [126/2000] - Loss: 41360.1406\n",
      "Epoch [127/2000] - Loss: 17989.8066\n",
      "Epoch [128/2000] - Loss: 198735.7188\n",
      "Epoch [129/2000] - Loss: 59265.5781\n",
      "Epoch [130/2000] - Loss: 10149.5684\n",
      "Epoch [131/2000] - Loss: 5013.6128\n",
      "Epoch [132/2000] - Loss: 7805.2363\n",
      "Epoch [133/2000] - Loss: 9246.7588\n",
      "Epoch [134/2000] - Loss: 243389.8750\n",
      "Epoch [135/2000] - Loss: 13379.7920\n",
      "Epoch [136/2000] - Loss: 6379.4053\n",
      "Epoch [137/2000] - Loss: 29140.6250\n",
      "Epoch [138/2000] - Loss: 19378.9668\n",
      "Epoch [139/2000] - Loss: 3704.1240\n",
      "Epoch [140/2000] - Loss: 2499.0039\n",
      "Epoch [141/2000] - Loss: 14765.8516\n",
      "Epoch [142/2000] - Loss: 178678.2031\n",
      "Epoch [143/2000] - Loss: 162343.2344\n",
      "Epoch [144/2000] - Loss: 125423.9375\n",
      "Epoch [145/2000] - Loss: 326343.7812\n",
      "Epoch [146/2000] - Loss: 56062.9570\n",
      "Epoch [147/2000] - Loss: 94633.3984\n",
      "Epoch [148/2000] - Loss: 40155.4414\n",
      "Epoch [149/2000] - Loss: 48161.3125\n",
      "Epoch [150/2000] - Loss: 60534.5312\n",
      "Epoch [151/2000] - Loss: 131618.9219\n",
      "Epoch [152/2000] - Loss: 19984.6934\n",
      "Epoch [153/2000] - Loss: 9989328.0000\n",
      "Epoch [154/2000] - Loss: 32889.6641\n",
      "Epoch [155/2000] - Loss: 42640.4492\n",
      "Epoch [156/2000] - Loss: 62655.3906\n",
      "Epoch [157/2000] - Loss: 5439.6064\n",
      "Epoch [158/2000] - Loss: 45542.6055\n",
      "Epoch [159/2000] - Loss: 6253.5576\n",
      "Epoch [160/2000] - Loss: 9750.2988\n",
      "Epoch [161/2000] - Loss: 4642.2334\n",
      "Epoch [162/2000] - Loss: 616074.0000\n",
      "Epoch [163/2000] - Loss: 2832.8770\n",
      "Epoch [164/2000] - Loss: 64101.7773\n",
      "Epoch [165/2000] - Loss: 30768.8242\n",
      "Epoch [166/2000] - Loss: 57921.2227\n",
      "Epoch [167/2000] - Loss: 26971.8984\n",
      "Epoch [168/2000] - Loss: 17256.5449\n",
      "Epoch [169/2000] - Loss: 13706.5508\n",
      "Epoch [170/2000] - Loss: 36042.5156\n",
      "Epoch [171/2000] - Loss: 31004.0996\n",
      "Epoch [172/2000] - Loss: 163274.7344\n",
      "Epoch [173/2000] - Loss: 51800.4609\n",
      "Epoch [174/2000] - Loss: 58800.8320\n",
      "Epoch [175/2000] - Loss: 545899.8125\n",
      "Epoch [176/2000] - Loss: 4132.8550\n",
      "Epoch [177/2000] - Loss: 20848.7324\n",
      "Epoch [178/2000] - Loss: 9265.5791\n",
      "Epoch [179/2000] - Loss: 5812.0645\n",
      "Epoch [180/2000] - Loss: 33126.3594\n",
      "Epoch [181/2000] - Loss: 4941.0674\n",
      "Epoch [182/2000] - Loss: 1914.6229\n",
      "Epoch [183/2000] - Loss: 547478.9375\n",
      "Epoch [184/2000] - Loss: 27943.0312\n",
      "Epoch [185/2000] - Loss: 24976.5527\n",
      "Epoch [186/2000] - Loss: 11804.3818\n",
      "Epoch [187/2000] - Loss: 13111.4258\n",
      "Epoch [188/2000] - Loss: 11890.4844\n",
      "Epoch [189/2000] - Loss: 26165.7188\n",
      "Epoch [190/2000] - Loss: 153035.7188\n",
      "Epoch [191/2000] - Loss: 52449.5234\n",
      "Epoch [192/2000] - Loss: 268877.4062\n",
      "Epoch [193/2000] - Loss: 8416.4951\n",
      "Epoch [194/2000] - Loss: 12738.3691\n",
      "Epoch [195/2000] - Loss: 21459.6113\n",
      "Epoch [196/2000] - Loss: 48191.1250\n",
      "Epoch [197/2000] - Loss: 45738.6523\n",
      "Epoch [198/2000] - Loss: 8202.1055\n",
      "Epoch [199/2000] - Loss: 9379.6172\n",
      "Epoch [200/2000] - Loss: 1713.9958\n",
      "Epoch [201/2000] - Loss: 394.7125\n",
      "Epoch [202/2000] - Loss: 2496.3987\n",
      "Epoch [203/2000] - Loss: 33959.9414\n",
      "Epoch [204/2000] - Loss: 642.9860\n",
      "Epoch [205/2000] - Loss: 1187.0267\n",
      "Epoch [206/2000] - Loss: 2144.0845\n",
      "Epoch [207/2000] - Loss: 450.6631\n",
      "Epoch [208/2000] - Loss: 264.0319\n",
      "Epoch [209/2000] - Loss: 217.7249\n",
      "Epoch [210/2000] - Loss: 766.8940\n",
      "Epoch [211/2000] - Loss: 803.4658\n",
      "Epoch [212/2000] - Loss: 643.1904\n",
      "Epoch [213/2000] - Loss: 143.9360\n",
      "Epoch [214/2000] - Loss: 3648.9895\n",
      "Epoch [215/2000] - Loss: 402.0105\n",
      "Epoch [216/2000] - Loss: 655.5974\n",
      "Epoch [217/2000] - Loss: 1006.1626\n",
      "Epoch [218/2000] - Loss: 1384.5813\n",
      "Epoch [219/2000] - Loss: 148.0906\n",
      "Epoch [220/2000] - Loss: 366.3154\n",
      "Epoch [221/2000] - Loss: 1089.8571\n",
      "Epoch [222/2000] - Loss: 1520.9744\n",
      "Epoch [223/2000] - Loss: 223.4056\n",
      "Epoch [224/2000] - Loss: 3258.4436\n",
      "Epoch [225/2000] - Loss: 110.7555\n",
      "Epoch [226/2000] - Loss: 194.5861\n",
      "Epoch [227/2000] - Loss: 127.2922\n",
      "Epoch [228/2000] - Loss: 81.6244\n",
      "Epoch [229/2000] - Loss: 87.4780\n",
      "Epoch [230/2000] - Loss: 298.5160\n",
      "Epoch [231/2000] - Loss: 441.4394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [232/2000] - Loss: 199.5658\n",
      "Epoch [233/2000] - Loss: 1372.4590\n",
      "Epoch [234/2000] - Loss: 249.4496\n",
      "Epoch [235/2000] - Loss: 399.6759\n",
      "Epoch [236/2000] - Loss: 397.0748\n",
      "Epoch [237/2000] - Loss: 4426.8799\n",
      "Epoch [238/2000] - Loss: 577.8541\n",
      "Epoch [239/2000] - Loss: 154.7923\n",
      "Epoch [240/2000] - Loss: 649.4290\n",
      "Epoch [241/2000] - Loss: 8539.9160\n",
      "Epoch [242/2000] - Loss: 256.0403\n",
      "Epoch [243/2000] - Loss: 101.4964\n",
      "Epoch [244/2000] - Loss: 82.0552\n",
      "Epoch [245/2000] - Loss: 59.1421\n",
      "Epoch [246/2000] - Loss: 91.6049\n",
      "Epoch [247/2000] - Loss: 65.7223\n",
      "Epoch [248/2000] - Loss: 58.9507\n",
      "Epoch [249/2000] - Loss: 1147.9728\n",
      "Epoch [250/2000] - Loss: 103.5647\n",
      "Epoch [251/2000] - Loss: 46.5411\n",
      "Epoch [252/2000] - Loss: 223.1834\n",
      "Epoch [253/2000] - Loss: 81.7308\n",
      "Epoch [254/2000] - Loss: 62.1815\n",
      "Epoch [255/2000] - Loss: 54.3347\n",
      "Epoch [256/2000] - Loss: 44.3536\n",
      "Epoch [257/2000] - Loss: 45.4262\n",
      "Epoch [258/2000] - Loss: 56.5363\n",
      "Epoch [259/2000] - Loss: 57.0573\n",
      "Epoch [260/2000] - Loss: 36.5199\n",
      "Epoch [261/2000] - Loss: 1229.2611\n",
      "Epoch [262/2000] - Loss: 1138.0175\n",
      "Epoch [263/2000] - Loss: 412.2651\n",
      "Epoch [264/2000] - Loss: 409.3340\n",
      "Epoch [265/2000] - Loss: 378.5874\n",
      "Epoch [266/2000] - Loss: 359.6708\n",
      "Epoch [267/2000] - Loss: 459.9518\n",
      "Epoch [268/2000] - Loss: 1466.3752\n",
      "Epoch [269/2000] - Loss: 542.2213\n",
      "Epoch [270/2000] - Loss: 249.0354\n",
      "Epoch [271/2000] - Loss: 1138.6847\n",
      "Epoch [272/2000] - Loss: 3152.3716\n",
      "Epoch [273/2000] - Loss: 323.0278\n",
      "Epoch [274/2000] - Loss: 160.1537\n",
      "Epoch [275/2000] - Loss: 195.1190\n",
      "Epoch [276/2000] - Loss: 143.8754\n",
      "Epoch [277/2000] - Loss: 121.3460\n",
      "Epoch [278/2000] - Loss: 480.0576\n",
      "Epoch [279/2000] - Loss: 468.0046\n",
      "Epoch [280/2000] - Loss: 219.3979\n",
      "Epoch [281/2000] - Loss: 162.0354\n",
      "Epoch [282/2000] - Loss: 522.7289\n",
      "Epoch [283/2000] - Loss: 2640.0830\n",
      "Epoch [284/2000] - Loss: 650.9838\n",
      "Epoch [285/2000] - Loss: 77.1545\n",
      "Epoch [286/2000] - Loss: 255.7004\n",
      "Epoch [287/2000] - Loss: 76.5104\n",
      "Epoch [288/2000] - Loss: 55.2803\n",
      "Epoch [289/2000] - Loss: 118.4387\n",
      "Epoch [290/2000] - Loss: 131.6388\n",
      "Epoch [291/2000] - Loss: 50.4086\n",
      "Epoch [292/2000] - Loss: 192.7646\n",
      "Epoch [293/2000] - Loss: 35.6407\n",
      "Epoch [294/2000] - Loss: 273.8593\n",
      "Epoch [295/2000] - Loss: 138.6469\n",
      "Epoch [296/2000] - Loss: 85.1426\n",
      "Epoch [297/2000] - Loss: 92.4871\n",
      "Epoch [298/2000] - Loss: 79.7948\n",
      "Epoch [299/2000] - Loss: 84.3490\n",
      "Epoch [300/2000] - Loss: 998.8525\n",
      "Epoch [301/2000] - Loss: 54.7323\n",
      "Epoch [302/2000] - Loss: 171.2529\n",
      "Epoch [303/2000] - Loss: 148.0295\n",
      "Epoch [304/2000] - Loss: 623.4689\n",
      "Epoch [305/2000] - Loss: 66.5147\n",
      "Epoch [306/2000] - Loss: 148.9961\n",
      "Epoch [307/2000] - Loss: 172.3578\n",
      "Epoch [308/2000] - Loss: 46.6816\n",
      "Epoch [309/2000] - Loss: 49.6329\n",
      "Epoch [310/2000] - Loss: 64.0718\n",
      "Epoch [311/2000] - Loss: 91.4543\n",
      "Epoch [312/2000] - Loss: 151.3274\n",
      "Epoch [313/2000] - Loss: 34.3503\n",
      "Epoch [314/2000] - Loss: 54.7035\n",
      "Epoch [315/2000] - Loss: 32.9355\n",
      "Epoch [316/2000] - Loss: 40.3435\n",
      "Epoch [317/2000] - Loss: 50.8028\n",
      "Epoch [318/2000] - Loss: 29.0692\n",
      "Epoch [319/2000] - Loss: 24.4393\n",
      "Epoch [320/2000] - Loss: 54.3929\n",
      "Epoch [321/2000] - Loss: 34.6361\n",
      "Epoch [322/2000] - Loss: 67.2401\n",
      "Epoch [323/2000] - Loss: 112.3693\n",
      "Epoch [324/2000] - Loss: 108.8654\n",
      "Epoch [325/2000] - Loss: 56.5822\n",
      "Epoch [326/2000] - Loss: 67.2030\n",
      "Epoch [327/2000] - Loss: 31.4148\n",
      "Epoch [328/2000] - Loss: 115.9710\n",
      "Epoch [329/2000] - Loss: 37.4032\n",
      "Epoch [330/2000] - Loss: 28.5660\n",
      "Epoch [331/2000] - Loss: 44.2613\n",
      "Epoch [332/2000] - Loss: 34.5288\n",
      "Epoch [333/2000] - Loss: 27.0281\n",
      "Epoch [334/2000] - Loss: 294.0078\n",
      "Epoch [335/2000] - Loss: 54.7430\n",
      "Epoch [336/2000] - Loss: 52.5601\n",
      "Epoch [337/2000] - Loss: 124.3610\n",
      "Epoch [338/2000] - Loss: 75.4818\n",
      "Epoch [339/2000] - Loss: 24.1349\n",
      "Epoch [340/2000] - Loss: 18.4225\n",
      "Epoch [341/2000] - Loss: 28.7713\n",
      "Epoch [342/2000] - Loss: 28.0952\n",
      "Epoch [343/2000] - Loss: 17.0293\n",
      "Epoch [344/2000] - Loss: 25.1281\n",
      "Epoch [345/2000] - Loss: 31.3301\n",
      "Epoch [346/2000] - Loss: 18.9021\n",
      "Epoch [347/2000] - Loss: 38.0594\n",
      "Epoch [348/2000] - Loss: 44.2540\n",
      "Epoch [349/2000] - Loss: 13.4197\n",
      "Epoch [350/2000] - Loss: 26.6053\n",
      "Epoch [351/2000] - Loss: 21.6033\n",
      "Epoch [352/2000] - Loss: 18.2504\n",
      "Epoch [353/2000] - Loss: 26.4116\n",
      "Epoch [354/2000] - Loss: 21.0773\n",
      "Epoch [355/2000] - Loss: 12.6691\n",
      "Epoch [356/2000] - Loss: 22.2506\n",
      "Epoch [357/2000] - Loss: 13.4960\n",
      "Epoch [358/2000] - Loss: 26.6671\n",
      "Epoch [359/2000] - Loss: 24.5454\n",
      "Epoch [360/2000] - Loss: 19.6804\n",
      "Epoch [361/2000] - Loss: 13.9417\n",
      "Epoch [362/2000] - Loss: 38.2314\n",
      "Epoch [363/2000] - Loss: 50.3872\n",
      "Epoch [364/2000] - Loss: 24.9796\n",
      "Epoch [365/2000] - Loss: 29.1742\n",
      "Epoch [366/2000] - Loss: 32.7764\n",
      "Epoch [367/2000] - Loss: 18.9402\n",
      "Epoch [368/2000] - Loss: 33.7358\n",
      "Epoch [369/2000] - Loss: 23.1829\n",
      "Epoch [370/2000] - Loss: 32.9244\n",
      "Epoch [371/2000] - Loss: 46.7624\n",
      "Epoch [372/2000] - Loss: 16.9773\n",
      "Epoch [373/2000] - Loss: 14.8912\n",
      "Epoch [374/2000] - Loss: 18.6172\n",
      "Epoch [375/2000] - Loss: 23.9231\n",
      "Epoch [376/2000] - Loss: 25.2254\n",
      "Epoch [377/2000] - Loss: 13.0446\n",
      "Epoch [378/2000] - Loss: 17.1420\n",
      "Epoch [379/2000] - Loss: 15.0128\n",
      "Epoch [380/2000] - Loss: 32.2371\n",
      "Epoch [381/2000] - Loss: 25.1860\n",
      "Epoch [382/2000] - Loss: 17.4309\n",
      "Epoch [383/2000] - Loss: 31.5972\n",
      "Epoch [384/2000] - Loss: 71.0249\n",
      "Epoch [385/2000] - Loss: 47.0950\n",
      "Epoch [386/2000] - Loss: 29.6850\n",
      "Epoch [387/2000] - Loss: 38.3386\n",
      "Epoch [388/2000] - Loss: 26.5916\n",
      "Epoch [389/2000] - Loss: 17.6036\n",
      "Epoch [390/2000] - Loss: 23.6773\n",
      "Epoch [391/2000] - Loss: 25.9072\n",
      "Epoch [392/2000] - Loss: 25.1305\n",
      "Epoch [393/2000] - Loss: 29.8667\n",
      "Epoch [394/2000] - Loss: 19.6102\n",
      "Epoch [395/2000] - Loss: 23.4015\n",
      "Epoch [396/2000] - Loss: 37.9780\n",
      "Epoch [397/2000] - Loss: 17.1305\n",
      "Epoch [398/2000] - Loss: 21.3926\n",
      "Epoch [399/2000] - Loss: 14.1960\n",
      "Epoch [400/2000] - Loss: 20.1467\n",
      "Epoch [401/2000] - Loss: 13.1418\n",
      "Epoch [402/2000] - Loss: 21.4948\n",
      "Epoch [403/2000] - Loss: 16.5505\n",
      "Epoch [404/2000] - Loss: 14.6013\n",
      "Epoch [405/2000] - Loss: 13.9597\n",
      "Epoch [406/2000] - Loss: 37.0100\n",
      "Epoch [407/2000] - Loss: 31.9965\n",
      "Epoch [408/2000] - Loss: 26.4109\n",
      "Epoch [409/2000] - Loss: 20.5122\n",
      "Epoch [410/2000] - Loss: 16.1690\n",
      "Epoch [411/2000] - Loss: 31.7983\n",
      "Epoch [412/2000] - Loss: 16.8155\n",
      "Epoch [413/2000] - Loss: 14.0475\n",
      "Epoch [414/2000] - Loss: 32.2996\n",
      "Epoch [415/2000] - Loss: 32.1815\n",
      "Epoch [416/2000] - Loss: 19.5801\n",
      "Epoch [417/2000] - Loss: 17.5048\n",
      "Epoch [418/2000] - Loss: 30.1528\n",
      "Epoch [419/2000] - Loss: 23.8786\n",
      "Epoch [420/2000] - Loss: 9.1237\n",
      "Epoch [421/2000] - Loss: 16.1121\n",
      "Epoch [422/2000] - Loss: 18.3437\n",
      "Epoch [423/2000] - Loss: 13.7947\n",
      "Epoch [424/2000] - Loss: 14.1621\n",
      "Epoch [425/2000] - Loss: 9.3953\n",
      "Epoch [426/2000] - Loss: 12.1777\n",
      "Epoch [427/2000] - Loss: 8.2350\n",
      "Epoch [428/2000] - Loss: 16.4861\n",
      "Epoch [429/2000] - Loss: 30.7525\n",
      "Epoch [430/2000] - Loss: 25.2435\n",
      "Epoch [431/2000] - Loss: 17.8772\n",
      "Epoch [432/2000] - Loss: 12.2493\n",
      "Epoch [433/2000] - Loss: 16.8637\n",
      "Epoch [434/2000] - Loss: 20.9450\n",
      "Epoch [435/2000] - Loss: 11.1698\n",
      "Epoch [436/2000] - Loss: 20.8348\n",
      "Epoch [437/2000] - Loss: 27.9766\n",
      "Epoch [438/2000] - Loss: 12.0826\n",
      "Epoch [439/2000] - Loss: 19.2910\n",
      "Epoch [440/2000] - Loss: 20.1836\n",
      "Epoch [441/2000] - Loss: 13.7540\n",
      "Epoch [442/2000] - Loss: 13.2732\n",
      "Epoch [443/2000] - Loss: 13.8520\n",
      "Epoch [444/2000] - Loss: 14.3876\n",
      "Epoch [445/2000] - Loss: 14.7222\n",
      "Epoch [446/2000] - Loss: 18.0899\n",
      "Epoch [447/2000] - Loss: 16.1068\n",
      "Epoch [448/2000] - Loss: 14.3185\n",
      "Epoch [449/2000] - Loss: 19.2121\n",
      "Epoch [450/2000] - Loss: 17.6442\n",
      "Epoch [451/2000] - Loss: 15.3813\n",
      "Epoch [452/2000] - Loss: 12.2471\n",
      "Epoch [453/2000] - Loss: 16.8447\n",
      "Epoch [454/2000] - Loss: 17.1124\n",
      "Epoch [455/2000] - Loss: 17.3654\n",
      "Epoch [456/2000] - Loss: 32.8997\n",
      "Epoch [457/2000] - Loss: 13.6994\n",
      "Epoch [458/2000] - Loss: 18.7979\n",
      "Epoch [459/2000] - Loss: 19.6325\n",
      "Epoch [460/2000] - Loss: 12.0520\n",
      "Epoch [461/2000] - Loss: 15.7962\n",
      "Epoch [462/2000] - Loss: 18.1779\n",
      "Epoch [463/2000] - Loss: 9.9836\n",
      "Epoch [464/2000] - Loss: 11.2697\n",
      "Epoch [465/2000] - Loss: 9.2534\n",
      "Epoch [466/2000] - Loss: 15.3154\n",
      "Epoch [467/2000] - Loss: 27.0515\n",
      "Epoch [468/2000] - Loss: 13.6775\n",
      "Epoch [469/2000] - Loss: 13.3176\n",
      "Epoch [470/2000] - Loss: 45.3353\n",
      "Epoch [471/2000] - Loss: 10.9329\n",
      "Epoch [472/2000] - Loss: 14.6099\n",
      "Epoch [473/2000] - Loss: 14.3792\n",
      "Epoch [474/2000] - Loss: 7.3592\n",
      "Epoch [475/2000] - Loss: 19.7974\n",
      "Epoch [476/2000] - Loss: 12.8300\n",
      "Epoch [477/2000] - Loss: 12.6548\n",
      "Epoch [478/2000] - Loss: 15.1174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [479/2000] - Loss: 10.6408\n",
      "Epoch [480/2000] - Loss: 22.8915\n",
      "Epoch [481/2000] - Loss: 16.6214\n",
      "Epoch [482/2000] - Loss: 19.9986\n",
      "Epoch [483/2000] - Loss: 16.0804\n",
      "Epoch [484/2000] - Loss: 33.5965\n",
      "Epoch [485/2000] - Loss: 22.4754\n",
      "Epoch [486/2000] - Loss: 10.0337\n",
      "Epoch [487/2000] - Loss: 16.2638\n",
      "Epoch [488/2000] - Loss: 11.6357\n",
      "Epoch [489/2000] - Loss: 25.0502\n",
      "Epoch [490/2000] - Loss: 16.3561\n",
      "Epoch [491/2000] - Loss: 13.0063\n",
      "Epoch [492/2000] - Loss: 16.9706\n",
      "Epoch [493/2000] - Loss: 12.5100\n",
      "Epoch [494/2000] - Loss: 15.3275\n",
      "Epoch [495/2000] - Loss: 7.8989\n",
      "Epoch [496/2000] - Loss: 11.9215\n",
      "Epoch [497/2000] - Loss: 12.2323\n",
      "Epoch [498/2000] - Loss: 16.5133\n",
      "Epoch [499/2000] - Loss: 26.9326\n",
      "Epoch [500/2000] - Loss: 10.0406\n",
      "Epoch [501/2000] - Loss: 9.4683\n",
      "Epoch [502/2000] - Loss: 13.7105\n",
      "Epoch [503/2000] - Loss: 8.1181\n",
      "Epoch [504/2000] - Loss: 24.2997\n",
      "Epoch [505/2000] - Loss: 12.0966\n",
      "Epoch [506/2000] - Loss: 9.4271\n",
      "Epoch [507/2000] - Loss: 12.5209\n",
      "Epoch [508/2000] - Loss: 14.5035\n",
      "Epoch [509/2000] - Loss: 15.9873\n",
      "Epoch [510/2000] - Loss: 15.1516\n",
      "Epoch [511/2000] - Loss: 12.8684\n",
      "Epoch [512/2000] - Loss: 13.3184\n",
      "Epoch [513/2000] - Loss: 13.2815\n",
      "Epoch [514/2000] - Loss: 26.2723\n",
      "Epoch [515/2000] - Loss: 12.4843\n",
      "Epoch [516/2000] - Loss: 21.6029\n",
      "Epoch [517/2000] - Loss: 14.3522\n",
      "Epoch [518/2000] - Loss: 9.9392\n",
      "Epoch [519/2000] - Loss: 15.4549\n",
      "Epoch [520/2000] - Loss: 12.6175\n",
      "Epoch [521/2000] - Loss: 14.0031\n",
      "Epoch [522/2000] - Loss: 13.6272\n",
      "Epoch [523/2000] - Loss: 9.4686\n",
      "Epoch [524/2000] - Loss: 12.5625\n",
      "Epoch [525/2000] - Loss: 16.8470\n",
      "Epoch [526/2000] - Loss: 22.8986\n",
      "Epoch [527/2000] - Loss: 16.1035\n",
      "Epoch [528/2000] - Loss: 25.4866\n",
      "Epoch [529/2000] - Loss: 9.3030\n",
      "Epoch [530/2000] - Loss: 13.1983\n",
      "Epoch [531/2000] - Loss: 11.2190\n",
      "Epoch [532/2000] - Loss: 10.1770\n",
      "Epoch [533/2000] - Loss: 10.8956\n",
      "Epoch [534/2000] - Loss: 24.4617\n",
      "Epoch [535/2000] - Loss: 11.5360\n",
      "Epoch [536/2000] - Loss: 8.7960\n",
      "Epoch [537/2000] - Loss: 18.0594\n",
      "Epoch [538/2000] - Loss: 14.0774\n",
      "Epoch [539/2000] - Loss: 20.5340\n",
      "Epoch [540/2000] - Loss: 21.1944\n",
      "Epoch [541/2000] - Loss: 15.4366\n",
      "Epoch [542/2000] - Loss: 12.0645\n",
      "Epoch [543/2000] - Loss: 7.6991\n",
      "Epoch [544/2000] - Loss: 36.6927\n",
      "Epoch [545/2000] - Loss: 14.7723\n",
      "Epoch [546/2000] - Loss: 26.7777\n",
      "Epoch [547/2000] - Loss: 6.8952\n",
      "Epoch [548/2000] - Loss: 25.2829\n",
      "Epoch [549/2000] - Loss: 19.2256\n",
      "Epoch [550/2000] - Loss: 11.8564\n",
      "Epoch [551/2000] - Loss: 10.3160\n",
      "Epoch [552/2000] - Loss: 10.0567\n",
      "Epoch [553/2000] - Loss: 5.8301\n",
      "Epoch [554/2000] - Loss: 7.3040\n",
      "Epoch [555/2000] - Loss: 16.7654\n",
      "Epoch [556/2000] - Loss: 14.7810\n",
      "Epoch [557/2000] - Loss: 10.4125\n",
      "Epoch [558/2000] - Loss: 10.9250\n",
      "Epoch [559/2000] - Loss: 11.0110\n",
      "Epoch [560/2000] - Loss: 8.8660\n",
      "Epoch [561/2000] - Loss: 7.8663\n",
      "Epoch [562/2000] - Loss: 13.8358\n",
      "Epoch [563/2000] - Loss: 9.9486\n",
      "Epoch [564/2000] - Loss: 21.2087\n",
      "Epoch [565/2000] - Loss: 12.7066\n",
      "Epoch [566/2000] - Loss: 14.2574\n",
      "Epoch [567/2000] - Loss: 14.5350\n",
      "Epoch [568/2000] - Loss: 14.0012\n",
      "Epoch [569/2000] - Loss: 14.3164\n",
      "Epoch [570/2000] - Loss: 9.5801\n",
      "Epoch [571/2000] - Loss: 18.1086\n",
      "Epoch [572/2000] - Loss: 7.3349\n",
      "Epoch [573/2000] - Loss: 8.1717\n",
      "Epoch [574/2000] - Loss: 9.2791\n",
      "Epoch [575/2000] - Loss: 15.9353\n",
      "Epoch [576/2000] - Loss: 14.8626\n",
      "Epoch [577/2000] - Loss: 17.2718\n",
      "Epoch [578/2000] - Loss: 13.5137\n",
      "Epoch [579/2000] - Loss: 16.0922\n",
      "Epoch [580/2000] - Loss: 19.1653\n",
      "Epoch [581/2000] - Loss: 11.4474\n",
      "Epoch [582/2000] - Loss: 14.4506\n",
      "Epoch [583/2000] - Loss: 17.2598\n",
      "Epoch [584/2000] - Loss: 15.7657\n",
      "Epoch [585/2000] - Loss: 8.9435\n",
      "Epoch [586/2000] - Loss: 10.4801\n",
      "Epoch [587/2000] - Loss: 16.5365\n",
      "Epoch [588/2000] - Loss: 7.2335\n",
      "Epoch [589/2000] - Loss: 6.1977\n",
      "Epoch [590/2000] - Loss: 7.6291\n",
      "Epoch [591/2000] - Loss: 21.7670\n",
      "Epoch [592/2000] - Loss: 15.4136\n",
      "Epoch [593/2000] - Loss: 11.3265\n",
      "Epoch [594/2000] - Loss: 9.1629\n",
      "Epoch [595/2000] - Loss: 16.7149\n",
      "Epoch [596/2000] - Loss: 29.9236\n",
      "Epoch [597/2000] - Loss: 19.9531\n",
      "Epoch [598/2000] - Loss: 16.9494\n",
      "Epoch [599/2000] - Loss: 11.7944\n",
      "Epoch [600/2000] - Loss: 24.6659\n",
      "Epoch [601/2000] - Loss: 23.5705\n",
      "Epoch [602/2000] - Loss: 9.0937\n",
      "Epoch [603/2000] - Loss: 11.4890\n",
      "Epoch [604/2000] - Loss: 11.0938\n",
      "Epoch [605/2000] - Loss: 18.2319\n",
      "Epoch [606/2000] - Loss: 12.4926\n",
      "Epoch [607/2000] - Loss: 15.9111\n",
      "Epoch [608/2000] - Loss: 10.8798\n",
      "Epoch [609/2000] - Loss: 16.5193\n",
      "Epoch [610/2000] - Loss: 13.5492\n",
      "Epoch [611/2000] - Loss: 12.2212\n",
      "Epoch [612/2000] - Loss: 9.6430\n",
      "Epoch [613/2000] - Loss: 8.7431\n",
      "Epoch [614/2000] - Loss: 22.3220\n",
      "Epoch [615/2000] - Loss: 9.9287\n",
      "Epoch [616/2000] - Loss: 6.9137\n",
      "Epoch [617/2000] - Loss: 15.0746\n",
      "Epoch [618/2000] - Loss: 15.3780\n",
      "Epoch [619/2000] - Loss: 8.4958\n",
      "Epoch [620/2000] - Loss: 27.2923\n",
      "Epoch [621/2000] - Loss: 16.4889\n",
      "Epoch [622/2000] - Loss: 17.6810\n",
      "Epoch [623/2000] - Loss: 8.8800\n",
      "Epoch [624/2000] - Loss: 15.2314\n",
      "Epoch [625/2000] - Loss: 9.0045\n",
      "Epoch [626/2000] - Loss: 10.1174\n",
      "Epoch [627/2000] - Loss: 13.3750\n",
      "Epoch [628/2000] - Loss: 14.7677\n",
      "Epoch [629/2000] - Loss: 10.0160\n",
      "Epoch [630/2000] - Loss: 8.3072\n",
      "Epoch [631/2000] - Loss: 20.0017\n",
      "Epoch [632/2000] - Loss: 21.4919\n",
      "Epoch [633/2000] - Loss: 11.9782\n",
      "Epoch [634/2000] - Loss: 11.7339\n",
      "Epoch [635/2000] - Loss: 12.7526\n",
      "Epoch [636/2000] - Loss: 29.1639\n",
      "Epoch [637/2000] - Loss: 10.0926\n",
      "Epoch [638/2000] - Loss: 16.7016\n",
      "Epoch [639/2000] - Loss: 6.5323\n",
      "Epoch [640/2000] - Loss: 7.3251\n",
      "Epoch [641/2000] - Loss: 11.0945\n",
      "Epoch [642/2000] - Loss: 17.5707\n",
      "Epoch [643/2000] - Loss: 21.7733\n",
      "Epoch [644/2000] - Loss: 21.3504\n",
      "Epoch [645/2000] - Loss: 20.0353\n",
      "Epoch [646/2000] - Loss: 6.1560\n",
      "Epoch [647/2000] - Loss: 8.5370\n",
      "Epoch [648/2000] - Loss: 18.5448\n",
      "Epoch [649/2000] - Loss: 29.5830\n",
      "Epoch [650/2000] - Loss: 22.3458\n",
      "Epoch [651/2000] - Loss: 15.9939\n",
      "Epoch [652/2000] - Loss: 10.2701\n",
      "Epoch [653/2000] - Loss: 12.2108\n",
      "Epoch [654/2000] - Loss: 11.6235\n",
      "Epoch [655/2000] - Loss: 15.9538\n",
      "Epoch [656/2000] - Loss: 14.1192\n",
      "Epoch [657/2000] - Loss: 27.3850\n",
      "Epoch [658/2000] - Loss: 11.7437\n",
      "Epoch [659/2000] - Loss: 8.3106\n",
      "Epoch [660/2000] - Loss: 10.6124\n",
      "Epoch [661/2000] - Loss: 12.1054\n",
      "Epoch [662/2000] - Loss: 21.8760\n",
      "Epoch [663/2000] - Loss: 11.2399\n",
      "Epoch [664/2000] - Loss: 9.6147\n",
      "Epoch [665/2000] - Loss: 16.1642\n",
      "Epoch [666/2000] - Loss: 21.7343\n",
      "Epoch [667/2000] - Loss: 7.8913\n",
      "Epoch [668/2000] - Loss: 8.9400\n",
      "Epoch [669/2000] - Loss: 20.7152\n",
      "Epoch [670/2000] - Loss: 10.6715\n",
      "Epoch [671/2000] - Loss: 22.1087\n",
      "Epoch [672/2000] - Loss: 26.7353\n",
      "Epoch [673/2000] - Loss: 7.7172\n",
      "Epoch [674/2000] - Loss: 16.5825\n",
      "Epoch [675/2000] - Loss: 8.0926\n",
      "Epoch [676/2000] - Loss: 8.0372\n",
      "Epoch [677/2000] - Loss: 29.4976\n",
      "Epoch [678/2000] - Loss: 5.5686\n",
      "Epoch [679/2000] - Loss: 8.3164\n",
      "Epoch [680/2000] - Loss: 28.3495\n",
      "Epoch [681/2000] - Loss: 7.6176\n",
      "Epoch [682/2000] - Loss: 13.5224\n",
      "Epoch [683/2000] - Loss: 7.5544\n",
      "Epoch [684/2000] - Loss: 18.3995\n",
      "Epoch [685/2000] - Loss: 11.3840\n",
      "Epoch [686/2000] - Loss: 11.3783\n",
      "Epoch [687/2000] - Loss: 11.9325\n",
      "Epoch [688/2000] - Loss: 12.4941\n",
      "Epoch [689/2000] - Loss: 16.5288\n",
      "Epoch [690/2000] - Loss: 7.9552\n",
      "Epoch [691/2000] - Loss: 18.1763\n",
      "Epoch [692/2000] - Loss: 10.0935\n",
      "Epoch [693/2000] - Loss: 26.2644\n",
      "Epoch [694/2000] - Loss: 7.8558\n",
      "Epoch [695/2000] - Loss: 12.2166\n",
      "Epoch [696/2000] - Loss: 9.8162\n",
      "Epoch [697/2000] - Loss: 8.6000\n",
      "Epoch [698/2000] - Loss: 23.3757\n",
      "Epoch [699/2000] - Loss: 5.1154\n",
      "Epoch [700/2000] - Loss: 19.9951\n",
      "Epoch [701/2000] - Loss: 15.2232\n",
      "Epoch [702/2000] - Loss: 20.2578\n",
      "Epoch [703/2000] - Loss: 10.7361\n",
      "Epoch [704/2000] - Loss: 12.5412\n",
      "Epoch [705/2000] - Loss: 17.4865\n",
      "Epoch [706/2000] - Loss: 16.2252\n",
      "Epoch [707/2000] - Loss: 14.8838\n",
      "Epoch [708/2000] - Loss: 11.6150\n",
      "Epoch [709/2000] - Loss: 3.7890\n",
      "Epoch [710/2000] - Loss: 13.9186\n",
      "Epoch [711/2000] - Loss: 11.0476\n",
      "Epoch [712/2000] - Loss: 17.9511\n",
      "Epoch [713/2000] - Loss: 15.5564\n",
      "Epoch [714/2000] - Loss: 11.1401\n",
      "Epoch [715/2000] - Loss: 15.8798\n",
      "Epoch [716/2000] - Loss: 10.5046\n",
      "Epoch [717/2000] - Loss: 12.3165\n",
      "Epoch [718/2000] - Loss: 11.2477\n",
      "Epoch [719/2000] - Loss: 9.0778\n",
      "Epoch [720/2000] - Loss: 20.2635\n",
      "Epoch [721/2000] - Loss: 6.8249\n",
      "Epoch [722/2000] - Loss: 9.3091\n",
      "Epoch [723/2000] - Loss: 17.6068\n",
      "Epoch [724/2000] - Loss: 14.6472\n",
      "Epoch [725/2000] - Loss: 15.5930\n",
      "Epoch [726/2000] - Loss: 9.8494\n",
      "Epoch [727/2000] - Loss: 19.4907\n",
      "Epoch [728/2000] - Loss: 10.2091\n",
      "Epoch [729/2000] - Loss: 16.9839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [730/2000] - Loss: 24.7140\n",
      "Epoch [731/2000] - Loss: 10.2510\n",
      "Epoch [732/2000] - Loss: 12.7323\n",
      "Epoch [733/2000] - Loss: 15.2136\n",
      "Epoch [734/2000] - Loss: 8.6889\n",
      "Epoch [735/2000] - Loss: 6.9659\n",
      "Epoch [736/2000] - Loss: 19.6218\n",
      "Epoch [737/2000] - Loss: 15.2367\n",
      "Epoch [738/2000] - Loss: 6.1884\n",
      "Epoch [739/2000] - Loss: 10.1300\n",
      "Epoch [740/2000] - Loss: 16.1489\n",
      "Epoch [741/2000] - Loss: 9.8726\n",
      "Epoch [742/2000] - Loss: 15.3884\n",
      "Epoch [743/2000] - Loss: 6.9382\n",
      "Epoch [744/2000] - Loss: 17.2096\n",
      "Epoch [745/2000] - Loss: 27.2027\n",
      "Epoch [746/2000] - Loss: 19.3757\n",
      "Epoch [747/2000] - Loss: 8.9620\n",
      "Epoch [748/2000] - Loss: 16.7317\n",
      "Epoch [749/2000] - Loss: 11.3506\n",
      "Epoch [750/2000] - Loss: 14.7881\n",
      "Epoch [751/2000] - Loss: 12.7299\n",
      "Epoch [752/2000] - Loss: 15.2689\n",
      "Epoch [753/2000] - Loss: 15.3609\n",
      "Epoch [754/2000] - Loss: 9.1584\n",
      "Epoch [755/2000] - Loss: 13.4644\n",
      "Epoch [756/2000] - Loss: 21.9752\n",
      "Epoch [757/2000] - Loss: 11.5290\n",
      "Epoch [758/2000] - Loss: 14.9610\n",
      "Epoch [759/2000] - Loss: 22.2672\n",
      "Epoch [760/2000] - Loss: 16.8133\n",
      "Epoch [761/2000] - Loss: 19.7442\n",
      "Epoch [762/2000] - Loss: 8.6485\n",
      "Epoch [763/2000] - Loss: 6.9147\n",
      "Epoch [764/2000] - Loss: 10.0799\n",
      "Epoch [765/2000] - Loss: 18.1330\n",
      "Epoch [766/2000] - Loss: 17.6403\n",
      "Epoch [767/2000] - Loss: 14.6598\n",
      "Epoch [768/2000] - Loss: 8.9285\n",
      "Epoch [769/2000] - Loss: 7.1528\n",
      "Epoch [770/2000] - Loss: 6.7084\n",
      "Epoch [771/2000] - Loss: 15.3573\n",
      "Epoch [772/2000] - Loss: 11.7573\n",
      "Epoch [773/2000] - Loss: 12.9577\n",
      "Epoch [774/2000] - Loss: 12.3701\n",
      "Epoch [775/2000] - Loss: 21.0275\n",
      "Epoch [776/2000] - Loss: 28.5125\n",
      "Epoch [777/2000] - Loss: 21.3921\n",
      "Epoch [778/2000] - Loss: 15.9505\n",
      "Epoch [779/2000] - Loss: 13.9053\n",
      "Epoch [780/2000] - Loss: 11.6676\n",
      "Epoch [781/2000] - Loss: 17.7102\n",
      "Epoch [782/2000] - Loss: 7.5777\n",
      "Epoch [783/2000] - Loss: 9.8925\n",
      "Epoch [784/2000] - Loss: 12.7492\n",
      "Epoch [785/2000] - Loss: 16.8314\n",
      "Epoch [786/2000] - Loss: 12.8265\n",
      "Epoch [787/2000] - Loss: 12.6940\n",
      "Epoch [788/2000] - Loss: 12.6091\n",
      "Epoch [789/2000] - Loss: 11.6164\n",
      "Epoch [790/2000] - Loss: 18.0273\n",
      "Epoch [791/2000] - Loss: 9.6378\n",
      "Epoch [792/2000] - Loss: 26.5187\n",
      "Epoch [793/2000] - Loss: 16.7724\n",
      "Epoch [794/2000] - Loss: 22.7569\n",
      "Epoch [795/2000] - Loss: 21.2349\n",
      "Epoch [796/2000] - Loss: 14.1559\n",
      "Epoch [797/2000] - Loss: 15.4119\n",
      "Epoch [798/2000] - Loss: 11.9563\n",
      "Epoch [799/2000] - Loss: 9.6922\n",
      "Epoch [800/2000] - Loss: 7.3981\n",
      "Epoch [801/2000] - Loss: 18.1508\n",
      "Epoch [802/2000] - Loss: 14.0569\n",
      "Epoch [803/2000] - Loss: 15.7624\n",
      "Epoch [804/2000] - Loss: 13.5590\n",
      "Epoch [805/2000] - Loss: 11.2922\n",
      "Epoch [806/2000] - Loss: 8.3874\n",
      "Epoch [807/2000] - Loss: 21.4406\n",
      "Epoch [808/2000] - Loss: 19.3705\n",
      "Epoch [809/2000] - Loss: 18.3059\n",
      "Epoch [810/2000] - Loss: 8.8687\n",
      "Epoch [811/2000] - Loss: 17.7346\n",
      "Epoch [812/2000] - Loss: 14.1659\n",
      "Epoch [813/2000] - Loss: 9.4439\n",
      "Epoch [814/2000] - Loss: 25.8020\n",
      "Epoch [815/2000] - Loss: 19.4629\n",
      "Epoch [816/2000] - Loss: 8.1591\n",
      "Epoch [817/2000] - Loss: 15.3612\n",
      "Epoch [818/2000] - Loss: 14.1108\n",
      "Epoch [819/2000] - Loss: 15.2055\n",
      "Epoch [820/2000] - Loss: 11.7599\n",
      "Epoch [821/2000] - Loss: 18.4606\n",
      "Epoch [822/2000] - Loss: 14.0541\n",
      "Epoch [823/2000] - Loss: 16.8341\n",
      "Epoch [824/2000] - Loss: 8.7718\n",
      "Epoch [825/2000] - Loss: 11.4530\n",
      "Epoch [826/2000] - Loss: 16.0208\n",
      "Epoch [827/2000] - Loss: 8.3023\n",
      "Epoch [828/2000] - Loss: 14.6278\n",
      "Epoch [829/2000] - Loss: 12.9711\n",
      "Epoch [830/2000] - Loss: 12.9411\n",
      "Epoch [831/2000] - Loss: 11.2192\n",
      "Epoch [832/2000] - Loss: 8.3248\n",
      "Epoch [833/2000] - Loss: 13.1213\n",
      "Epoch [834/2000] - Loss: 17.3936\n",
      "Epoch [835/2000] - Loss: 20.1279\n",
      "Epoch [836/2000] - Loss: 7.4864\n",
      "Epoch [837/2000] - Loss: 9.4889\n",
      "Epoch [838/2000] - Loss: 11.6239\n",
      "Epoch [839/2000] - Loss: 6.4152\n",
      "Epoch [840/2000] - Loss: 15.9332\n",
      "Epoch [841/2000] - Loss: 11.8522\n",
      "Epoch [842/2000] - Loss: 9.7095\n",
      "Epoch [843/2000] - Loss: 13.7675\n",
      "Epoch [844/2000] - Loss: 20.5359\n",
      "Epoch [845/2000] - Loss: 11.1927\n",
      "Epoch [846/2000] - Loss: 4.9092\n",
      "Epoch [847/2000] - Loss: 15.4436\n",
      "Epoch [848/2000] - Loss: 11.3249\n",
      "Epoch [849/2000] - Loss: 19.2822\n",
      "Epoch [850/2000] - Loss: 11.3552\n",
      "Epoch [851/2000] - Loss: 12.1841\n",
      "Epoch [852/2000] - Loss: 16.8298\n",
      "Epoch [853/2000] - Loss: 16.8780\n",
      "Epoch [854/2000] - Loss: 7.7476\n",
      "Epoch [855/2000] - Loss: 13.6947\n",
      "Epoch [856/2000] - Loss: 12.5344\n",
      "Epoch [857/2000] - Loss: 16.8264\n",
      "Epoch [858/2000] - Loss: 10.9372\n",
      "Epoch [859/2000] - Loss: 7.4550\n",
      "Epoch [860/2000] - Loss: 13.1830\n",
      "Epoch [861/2000] - Loss: 6.7637\n",
      "Epoch [862/2000] - Loss: 15.1551\n",
      "Epoch [863/2000] - Loss: 10.5640\n",
      "Epoch [864/2000] - Loss: 17.3313\n",
      "Epoch [865/2000] - Loss: 13.4335\n",
      "Epoch [866/2000] - Loss: 3.6267\n",
      "Epoch [867/2000] - Loss: 24.4753\n",
      "Epoch [868/2000] - Loss: 16.6098\n",
      "Epoch [869/2000] - Loss: 12.1048\n",
      "Epoch [870/2000] - Loss: 9.3533\n",
      "Epoch [871/2000] - Loss: 17.7491\n",
      "Epoch [872/2000] - Loss: 9.2746\n",
      "Epoch [873/2000] - Loss: 20.7498\n",
      "Epoch [874/2000] - Loss: 24.4102\n",
      "Epoch [875/2000] - Loss: 10.2016\n",
      "Epoch [876/2000] - Loss: 11.8409\n",
      "Epoch [877/2000] - Loss: 6.1074\n",
      "Epoch [878/2000] - Loss: 10.6060\n",
      "Epoch [879/2000] - Loss: 20.9084\n",
      "Epoch [880/2000] - Loss: 9.5678\n",
      "Epoch [881/2000] - Loss: 27.6266\n",
      "Epoch [882/2000] - Loss: 9.6268\n",
      "Epoch [883/2000] - Loss: 8.2015\n",
      "Epoch [884/2000] - Loss: 16.0385\n",
      "Epoch [885/2000] - Loss: 16.8011\n",
      "Epoch [886/2000] - Loss: 10.3401\n",
      "Epoch [887/2000] - Loss: 9.4684\n",
      "Epoch [888/2000] - Loss: 10.1891\n",
      "Epoch [889/2000] - Loss: 14.4632\n",
      "Epoch [890/2000] - Loss: 10.7315\n",
      "Epoch [891/2000] - Loss: 12.0966\n",
      "Epoch [892/2000] - Loss: 8.4349\n",
      "Epoch [893/2000] - Loss: 12.2745\n",
      "Epoch [894/2000] - Loss: 14.3090\n",
      "Epoch [895/2000] - Loss: 8.2282\n",
      "Epoch [896/2000] - Loss: 6.2164\n",
      "Epoch [897/2000] - Loss: 9.4870\n",
      "Epoch [898/2000] - Loss: 12.1144\n",
      "Epoch [899/2000] - Loss: 14.2537\n",
      "Epoch [900/2000] - Loss: 9.2319\n",
      "Epoch [901/2000] - Loss: 16.9974\n",
      "Epoch [902/2000] - Loss: 10.0636\n",
      "Epoch [903/2000] - Loss: 11.8119\n",
      "Epoch [904/2000] - Loss: 8.9971\n",
      "Epoch [905/2000] - Loss: 11.9693\n",
      "Epoch [906/2000] - Loss: 8.4735\n",
      "Epoch [907/2000] - Loss: 9.6888\n",
      "Epoch [908/2000] - Loss: 12.0044\n",
      "Epoch [909/2000] - Loss: 21.5114\n",
      "Epoch [910/2000] - Loss: 10.0611\n",
      "Epoch [911/2000] - Loss: 17.9969\n",
      "Epoch [912/2000] - Loss: 6.8983\n",
      "Epoch [913/2000] - Loss: 9.3640\n",
      "Epoch [914/2000] - Loss: 12.6544\n",
      "Epoch [915/2000] - Loss: 13.6183\n",
      "Epoch [916/2000] - Loss: 17.9274\n",
      "Epoch [917/2000] - Loss: 18.7975\n",
      "Epoch [918/2000] - Loss: 9.7628\n",
      "Epoch [919/2000] - Loss: 13.9657\n",
      "Epoch [920/2000] - Loss: 14.1453\n",
      "Epoch [921/2000] - Loss: 6.2534\n",
      "Epoch [922/2000] - Loss: 8.4928\n",
      "Epoch [923/2000] - Loss: 14.7071\n",
      "Epoch [924/2000] - Loss: 15.1726\n",
      "Epoch [925/2000] - Loss: 9.6322\n",
      "Epoch [926/2000] - Loss: 9.0222\n",
      "Epoch [927/2000] - Loss: 8.7165\n",
      "Epoch [928/2000] - Loss: 17.6423\n",
      "Epoch [929/2000] - Loss: 10.1167\n",
      "Epoch [930/2000] - Loss: 12.0711\n",
      "Epoch [931/2000] - Loss: 12.0778\n",
      "Epoch [932/2000] - Loss: 14.8232\n",
      "Epoch [933/2000] - Loss: 17.2485\n",
      "Epoch [934/2000] - Loss: 6.6735\n",
      "Epoch [935/2000] - Loss: 6.7947\n",
      "Epoch [936/2000] - Loss: 10.2861\n",
      "Epoch [937/2000] - Loss: 14.0782\n",
      "Epoch [938/2000] - Loss: 9.7892\n",
      "Epoch [939/2000] - Loss: 11.1215\n",
      "Epoch [940/2000] - Loss: 18.9554\n",
      "Epoch [941/2000] - Loss: 14.1159\n",
      "Epoch [942/2000] - Loss: 14.5784\n",
      "Epoch [943/2000] - Loss: 15.5549\n",
      "Epoch [944/2000] - Loss: 12.8281\n",
      "Epoch [945/2000] - Loss: 9.1437\n",
      "Epoch [946/2000] - Loss: 5.5061\n",
      "Epoch [947/2000] - Loss: 25.2105\n",
      "Epoch [948/2000] - Loss: 11.1553\n",
      "Epoch [949/2000] - Loss: 26.7210\n",
      "Epoch [950/2000] - Loss: 11.4352\n",
      "Epoch [951/2000] - Loss: 10.2040\n",
      "Epoch [952/2000] - Loss: 6.4962\n",
      "Epoch [953/2000] - Loss: 5.5819\n",
      "Epoch [954/2000] - Loss: 13.6419\n",
      "Epoch [955/2000] - Loss: 14.7001\n",
      "Epoch [956/2000] - Loss: 21.3458\n",
      "Epoch [957/2000] - Loss: 3.8320\n",
      "Epoch [958/2000] - Loss: 8.1997\n",
      "Epoch [959/2000] - Loss: 10.7852\n",
      "Epoch [960/2000] - Loss: 5.4530\n",
      "Epoch [961/2000] - Loss: 8.3208\n",
      "Epoch [962/2000] - Loss: 10.9774\n",
      "Epoch [963/2000] - Loss: 6.8302\n",
      "Epoch [964/2000] - Loss: 17.1973\n",
      "Epoch [965/2000] - Loss: 17.2087\n",
      "Epoch [966/2000] - Loss: 19.3294\n",
      "Epoch [967/2000] - Loss: 18.6700\n",
      "Epoch [968/2000] - Loss: 16.9881\n",
      "Epoch [969/2000] - Loss: 7.0704\n",
      "Epoch [970/2000] - Loss: 13.5638\n",
      "Epoch [971/2000] - Loss: 15.6751\n",
      "Epoch [972/2000] - Loss: 23.5503\n",
      "Epoch [973/2000] - Loss: 11.2207\n",
      "Epoch [974/2000] - Loss: 10.7108\n",
      "Epoch [975/2000] - Loss: 13.6530\n",
      "Epoch [976/2000] - Loss: 8.4676\n",
      "Epoch [977/2000] - Loss: 16.3029\n",
      "Epoch [978/2000] - Loss: 15.3075\n",
      "Epoch [979/2000] - Loss: 11.1899\n",
      "Epoch [980/2000] - Loss: 21.1931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [981/2000] - Loss: 8.3759\n",
      "Epoch [982/2000] - Loss: 17.6457\n",
      "Epoch [983/2000] - Loss: 13.3289\n",
      "Epoch [984/2000] - Loss: 23.1266\n",
      "Epoch [985/2000] - Loss: 11.6420\n",
      "Epoch [986/2000] - Loss: 18.1823\n",
      "Epoch [987/2000] - Loss: 7.4028\n",
      "Epoch [988/2000] - Loss: 6.0129\n",
      "Epoch [989/2000] - Loss: 12.1236\n",
      "Epoch [990/2000] - Loss: 4.5403\n",
      "Epoch [991/2000] - Loss: 5.0376\n",
      "Epoch [992/2000] - Loss: 14.4365\n",
      "Epoch [993/2000] - Loss: 14.6672\n",
      "Epoch [994/2000] - Loss: 12.4372\n",
      "Epoch [995/2000] - Loss: 10.9489\n",
      "Epoch [996/2000] - Loss: 16.5549\n",
      "Epoch [997/2000] - Loss: 24.2046\n",
      "Epoch [998/2000] - Loss: 13.9351\n",
      "Epoch [999/2000] - Loss: 9.2462\n",
      "Epoch [1000/2000] - Loss: 8.7803\n",
      "Epoch [1001/2000] - Loss: 11.2086\n",
      "Epoch [1002/2000] - Loss: 9.7233\n",
      "Epoch [1003/2000] - Loss: 11.9198\n",
      "Epoch [1004/2000] - Loss: 7.5626\n",
      "Epoch [1005/2000] - Loss: 14.8271\n",
      "Epoch [1006/2000] - Loss: 27.7920\n",
      "Epoch [1007/2000] - Loss: 9.2679\n",
      "Epoch [1008/2000] - Loss: 13.1936\n",
      "Epoch [1009/2000] - Loss: 11.3213\n",
      "Epoch [1010/2000] - Loss: 8.8933\n",
      "Epoch [1011/2000] - Loss: 16.7499\n",
      "Epoch [1012/2000] - Loss: 10.2024\n",
      "Epoch [1013/2000] - Loss: 16.6364\n",
      "Epoch [1014/2000] - Loss: 21.5801\n",
      "Epoch [1015/2000] - Loss: 10.4762\n",
      "Epoch [1016/2000] - Loss: 10.6855\n",
      "Epoch [1017/2000] - Loss: 18.7281\n",
      "Epoch [1018/2000] - Loss: 18.3460\n",
      "Epoch [1019/2000] - Loss: 11.1148\n",
      "Epoch [1020/2000] - Loss: 11.9150\n",
      "Epoch [1021/2000] - Loss: 13.5523\n",
      "Epoch [1022/2000] - Loss: 12.2077\n",
      "Epoch [1023/2000] - Loss: 8.3046\n",
      "Epoch [1024/2000] - Loss: 12.4957\n",
      "Epoch [1025/2000] - Loss: 7.6468\n",
      "Epoch [1026/2000] - Loss: 11.8707\n",
      "Epoch [1027/2000] - Loss: 22.4362\n",
      "Epoch [1028/2000] - Loss: 9.1493\n",
      "Epoch [1029/2000] - Loss: 10.6663\n",
      "Epoch [1030/2000] - Loss: 21.0686\n",
      "Epoch [1031/2000] - Loss: 10.5637\n",
      "Epoch [1032/2000] - Loss: 11.5749\n",
      "Epoch [1033/2000] - Loss: 15.8407\n",
      "Epoch [1034/2000] - Loss: 14.7951\n",
      "Epoch [1035/2000] - Loss: 15.3145\n",
      "Epoch [1036/2000] - Loss: 17.0255\n",
      "Epoch [1037/2000] - Loss: 15.0488\n",
      "Epoch [1038/2000] - Loss: 23.0657\n",
      "Epoch [1039/2000] - Loss: 9.0648\n",
      "Epoch [1040/2000] - Loss: 14.8266\n",
      "Epoch [1041/2000] - Loss: 19.2421\n",
      "Epoch [1042/2000] - Loss: 3.1225\n",
      "Epoch [1043/2000] - Loss: 15.8528\n",
      "Epoch [1044/2000] - Loss: 10.3249\n",
      "Epoch [1045/2000] - Loss: 9.6277\n",
      "Epoch [1046/2000] - Loss: 13.3904\n",
      "Epoch [1047/2000] - Loss: 5.4710\n",
      "Epoch [1048/2000] - Loss: 21.4397\n",
      "Epoch [1049/2000] - Loss: 21.0114\n",
      "Epoch [1050/2000] - Loss: 12.3000\n",
      "Epoch [1051/2000] - Loss: 6.3882\n",
      "Epoch [1052/2000] - Loss: 7.5488\n",
      "Epoch [1053/2000] - Loss: 14.9559\n",
      "Epoch [1054/2000] - Loss: 11.0520\n",
      "Epoch [1055/2000] - Loss: 10.4695\n",
      "Epoch [1056/2000] - Loss: 17.7816\n",
      "Epoch [1057/2000] - Loss: 12.5292\n",
      "Epoch [1058/2000] - Loss: 11.4854\n",
      "Epoch [1059/2000] - Loss: 20.4746\n",
      "Epoch [1060/2000] - Loss: 15.2259\n",
      "Epoch [1061/2000] - Loss: 11.8776\n",
      "Epoch [1062/2000] - Loss: 10.1314\n",
      "Epoch [1063/2000] - Loss: 12.1930\n",
      "Epoch [1064/2000] - Loss: 13.8165\n",
      "Epoch [1065/2000] - Loss: 15.2105\n",
      "Epoch [1066/2000] - Loss: 5.8136\n",
      "Epoch [1067/2000] - Loss: 14.7955\n",
      "Epoch [1068/2000] - Loss: 12.6277\n",
      "Epoch [1069/2000] - Loss: 9.3990\n",
      "Epoch [1070/2000] - Loss: 22.3180\n",
      "Epoch [1071/2000] - Loss: 19.4160\n",
      "Epoch [1072/2000] - Loss: 19.4160\n",
      "Epoch [1073/2000] - Loss: 11.5778\n",
      "Epoch [1074/2000] - Loss: 17.2132\n",
      "Epoch [1075/2000] - Loss: 14.3836\n",
      "Epoch [1076/2000] - Loss: 13.3208\n",
      "Epoch [1077/2000] - Loss: 5.4831\n",
      "Epoch [1078/2000] - Loss: 16.3504\n",
      "Epoch [1079/2000] - Loss: 13.9288\n",
      "Epoch [1080/2000] - Loss: 12.1821\n",
      "Epoch [1081/2000] - Loss: 10.1137\n",
      "Epoch [1082/2000] - Loss: 13.2743\n",
      "Epoch [1083/2000] - Loss: 21.6806\n",
      "Epoch [1084/2000] - Loss: 18.8506\n",
      "Epoch [1085/2000] - Loss: 14.1078\n",
      "Epoch [1086/2000] - Loss: 10.5446\n",
      "Epoch [1087/2000] - Loss: 9.7405\n",
      "Epoch [1088/2000] - Loss: 9.2857\n",
      "Epoch [1089/2000] - Loss: 22.9296\n",
      "Epoch [1090/2000] - Loss: 11.3710\n",
      "Epoch [1091/2000] - Loss: 2.9956\n",
      "Epoch [1092/2000] - Loss: 16.6370\n",
      "Epoch [1093/2000] - Loss: 13.4303\n",
      "Epoch [1094/2000] - Loss: 22.2288\n",
      "Epoch [1095/2000] - Loss: 8.8933\n",
      "Epoch [1096/2000] - Loss: 6.5192\n",
      "Epoch [1097/2000] - Loss: 8.8683\n",
      "Epoch [1098/2000] - Loss: 8.9760\n",
      "Epoch [1099/2000] - Loss: 8.6119\n",
      "Epoch [1100/2000] - Loss: 5.8782\n",
      "Epoch [1101/2000] - Loss: 23.1528\n",
      "Epoch [1102/2000] - Loss: 4.5911\n",
      "Epoch [1103/2000] - Loss: 14.5340\n",
      "Epoch [1104/2000] - Loss: 12.0952\n",
      "Epoch [1105/2000] - Loss: 14.3018\n",
      "Epoch [1106/2000] - Loss: 17.3899\n",
      "Epoch [1107/2000] - Loss: 22.7710\n",
      "Epoch [1108/2000] - Loss: 15.0407\n",
      "Epoch [1109/2000] - Loss: 12.2205\n",
      "Epoch [1110/2000] - Loss: 9.0024\n",
      "Epoch [1111/2000] - Loss: 13.4988\n",
      "Epoch [1112/2000] - Loss: 8.6962\n",
      "Epoch [1113/2000] - Loss: 10.9104\n",
      "Epoch [1114/2000] - Loss: 19.9686\n",
      "Epoch [1115/2000] - Loss: 9.3000\n",
      "Epoch [1116/2000] - Loss: 5.1191\n",
      "Epoch [1117/2000] - Loss: 26.7429\n",
      "Epoch [1118/2000] - Loss: 18.9789\n",
      "Epoch [1119/2000] - Loss: 11.1153\n",
      "Epoch [1120/2000] - Loss: 13.5646\n",
      "Epoch [1121/2000] - Loss: 6.8545\n",
      "Epoch [1122/2000] - Loss: 10.9213\n",
      "Epoch [1123/2000] - Loss: 8.5384\n",
      "Epoch [1124/2000] - Loss: 13.7305\n",
      "Epoch [1125/2000] - Loss: 11.0003\n",
      "Epoch [1126/2000] - Loss: 9.7328\n",
      "Epoch [1127/2000] - Loss: 19.0262\n",
      "Epoch [1128/2000] - Loss: 15.5145\n",
      "Epoch [1129/2000] - Loss: 19.2841\n",
      "Epoch [1130/2000] - Loss: 21.8436\n",
      "Epoch [1131/2000] - Loss: 24.2131\n",
      "Epoch [1132/2000] - Loss: 9.6750\n",
      "Epoch [1133/2000] - Loss: 9.4744\n",
      "Epoch [1134/2000] - Loss: 21.8754\n",
      "Epoch [1135/2000] - Loss: 14.7868\n",
      "Epoch [1136/2000] - Loss: 18.4202\n",
      "Epoch [1137/2000] - Loss: 15.6092\n",
      "Epoch [1138/2000] - Loss: 7.8893\n",
      "Epoch [1139/2000] - Loss: 7.5508\n",
      "Epoch [1140/2000] - Loss: 6.6176\n",
      "Epoch [1141/2000] - Loss: 10.7440\n",
      "Epoch [1142/2000] - Loss: 9.1716\n",
      "Epoch [1143/2000] - Loss: 13.1284\n",
      "Epoch [1144/2000] - Loss: 25.9145\n",
      "Epoch [1145/2000] - Loss: 13.7523\n",
      "Epoch [1146/2000] - Loss: 13.9449\n",
      "Epoch [1147/2000] - Loss: 5.3339\n",
      "Epoch [1148/2000] - Loss: 10.3933\n",
      "Epoch [1149/2000] - Loss: 16.1743\n",
      "Epoch [1150/2000] - Loss: 16.9182\n",
      "Epoch [1151/2000] - Loss: 15.4830\n",
      "Epoch [1152/2000] - Loss: 19.3966\n",
      "Epoch [1153/2000] - Loss: 9.1119\n",
      "Epoch [1154/2000] - Loss: 8.1428\n",
      "Epoch [1155/2000] - Loss: 14.5265\n",
      "Epoch [1156/2000] - Loss: 12.1885\n",
      "Epoch [1157/2000] - Loss: 11.0066\n",
      "Epoch [1158/2000] - Loss: 19.0009\n",
      "Epoch [1159/2000] - Loss: 5.0070\n",
      "Epoch [1160/2000] - Loss: 6.1198\n",
      "Epoch [1161/2000] - Loss: 16.1074\n",
      "Epoch [1162/2000] - Loss: 13.0731\n",
      "Epoch [1163/2000] - Loss: 13.7370\n",
      "Epoch [1164/2000] - Loss: 12.1170\n",
      "Epoch [1165/2000] - Loss: 18.2841\n",
      "Epoch [1166/2000] - Loss: 17.8710\n",
      "Epoch [1167/2000] - Loss: 27.4108\n",
      "Epoch [1168/2000] - Loss: 15.9549\n",
      "Epoch [1169/2000] - Loss: 5.5432\n",
      "Epoch [1170/2000] - Loss: 13.7716\n",
      "Epoch [1171/2000] - Loss: 10.5227\n",
      "Epoch [1172/2000] - Loss: 18.7868\n",
      "Epoch [1173/2000] - Loss: 10.1528\n",
      "Epoch [1174/2000] - Loss: 12.3100\n",
      "Epoch [1175/2000] - Loss: 5.5032\n",
      "Epoch [1176/2000] - Loss: 14.9687\n",
      "Epoch [1177/2000] - Loss: 5.5023\n",
      "Epoch [1178/2000] - Loss: 11.2302\n",
      "Epoch [1179/2000] - Loss: 17.5924\n",
      "Epoch [1180/2000] - Loss: 12.6648\n",
      "Epoch [1181/2000] - Loss: 9.2081\n",
      "Epoch [1182/2000] - Loss: 14.7736\n",
      "Epoch [1183/2000] - Loss: 12.3114\n",
      "Epoch [1184/2000] - Loss: 47.2199\n",
      "Epoch [1185/2000] - Loss: 9.3763\n",
      "Epoch [1186/2000] - Loss: 13.4425\n",
      "Epoch [1187/2000] - Loss: 9.8563\n",
      "Epoch [1188/2000] - Loss: 13.3067\n",
      "Epoch [1189/2000] - Loss: 12.6925\n",
      "Epoch [1190/2000] - Loss: 9.9920\n",
      "Epoch [1191/2000] - Loss: 12.0945\n",
      "Epoch [1192/2000] - Loss: 7.7617\n",
      "Epoch [1193/2000] - Loss: 16.8808\n",
      "Epoch [1194/2000] - Loss: 5.3095\n",
      "Epoch [1195/2000] - Loss: 13.5181\n",
      "Epoch [1196/2000] - Loss: 10.1128\n",
      "Epoch [1197/2000] - Loss: 14.1258\n",
      "Epoch [1198/2000] - Loss: 9.0113\n",
      "Epoch [1199/2000] - Loss: 8.6433\n",
      "Epoch [1200/2000] - Loss: 12.0092\n",
      "Epoch [1201/2000] - Loss: 10.4973\n",
      "Epoch [1202/2000] - Loss: 10.7645\n",
      "Epoch [1203/2000] - Loss: 12.8739\n",
      "Epoch [1204/2000] - Loss: 13.8259\n",
      "Epoch [1205/2000] - Loss: 17.7346\n",
      "Epoch [1206/2000] - Loss: 16.6957\n",
      "Epoch [1207/2000] - Loss: 18.3647\n",
      "Epoch [1208/2000] - Loss: 7.4911\n",
      "Epoch [1209/2000] - Loss: 14.8161\n",
      "Epoch [1210/2000] - Loss: 19.5264\n",
      "Epoch [1211/2000] - Loss: 10.5821\n",
      "Epoch [1212/2000] - Loss: 14.0205\n",
      "Epoch [1213/2000] - Loss: 9.8960\n",
      "Epoch [1214/2000] - Loss: 15.0645\n",
      "Epoch [1215/2000] - Loss: 11.0268\n",
      "Epoch [1216/2000] - Loss: 19.4771\n",
      "Epoch [1217/2000] - Loss: 6.1192\n",
      "Epoch [1218/2000] - Loss: 21.4634\n",
      "Epoch [1219/2000] - Loss: 7.2161\n",
      "Epoch [1220/2000] - Loss: 8.7042\n",
      "Epoch [1221/2000] - Loss: 11.0451\n",
      "Epoch [1222/2000] - Loss: 16.3593\n",
      "Epoch [1223/2000] - Loss: 11.5981\n",
      "Epoch [1224/2000] - Loss: 12.1051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1225/2000] - Loss: 14.7301\n",
      "Epoch [1226/2000] - Loss: 9.9523\n",
      "Epoch [1227/2000] - Loss: 16.0778\n",
      "Epoch [1228/2000] - Loss: 8.5693\n",
      "Epoch [1229/2000] - Loss: 9.2111\n",
      "Epoch [1230/2000] - Loss: 14.8168\n",
      "Epoch [1231/2000] - Loss: 15.8418\n",
      "Epoch [1232/2000] - Loss: 10.8267\n",
      "Epoch [1233/2000] - Loss: 8.2194\n",
      "Epoch [1234/2000] - Loss: 18.8516\n",
      "Epoch [1235/2000] - Loss: 10.7719\n",
      "Epoch [1236/2000] - Loss: 6.6898\n",
      "Epoch [1237/2000] - Loss: 7.3984\n",
      "Epoch [1238/2000] - Loss: 8.9598\n",
      "Epoch [1239/2000] - Loss: 11.7616\n",
      "Epoch [1240/2000] - Loss: 12.4944\n",
      "Epoch [1241/2000] - Loss: 17.1273\n",
      "Epoch [1242/2000] - Loss: 6.7077\n",
      "Epoch [1243/2000] - Loss: 14.3592\n",
      "Epoch [1244/2000] - Loss: 10.3947\n",
      "Epoch [1245/2000] - Loss: 12.8369\n",
      "Epoch [1246/2000] - Loss: 10.5681\n",
      "Epoch [1247/2000] - Loss: 18.3927\n",
      "Epoch [1248/2000] - Loss: 10.1369\n",
      "Epoch [1249/2000] - Loss: 14.8279\n",
      "Epoch [1250/2000] - Loss: 10.3426\n",
      "Epoch [1251/2000] - Loss: 14.0149\n",
      "Epoch [1252/2000] - Loss: 19.9177\n",
      "Epoch [1253/2000] - Loss: 15.5983\n",
      "Epoch [1254/2000] - Loss: 16.7410\n",
      "Epoch [1255/2000] - Loss: 7.9163\n",
      "Epoch [1256/2000] - Loss: 16.6444\n",
      "Epoch [1257/2000] - Loss: 15.7699\n",
      "Epoch [1258/2000] - Loss: 20.0694\n",
      "Epoch [1259/2000] - Loss: 7.8220\n",
      "Epoch [1260/2000] - Loss: 13.5331\n",
      "Epoch [1261/2000] - Loss: 10.1420\n",
      "Epoch [1262/2000] - Loss: 4.0909\n",
      "Epoch [1263/2000] - Loss: 14.8067\n",
      "Epoch [1264/2000] - Loss: 11.5928\n",
      "Epoch [1265/2000] - Loss: 18.4738\n",
      "Epoch [1266/2000] - Loss: 3.9534\n",
      "Epoch [1267/2000] - Loss: 9.2523\n",
      "Epoch [1268/2000] - Loss: 8.3827\n",
      "Epoch [1269/2000] - Loss: 13.7368\n",
      "Epoch [1270/2000] - Loss: 17.7202\n",
      "Epoch [1271/2000] - Loss: 12.6669\n",
      "Epoch [1272/2000] - Loss: 10.5931\n",
      "Epoch [1273/2000] - Loss: 11.6465\n",
      "Epoch [1274/2000] - Loss: 9.4438\n",
      "Epoch [1275/2000] - Loss: 13.0610\n",
      "Epoch [1276/2000] - Loss: 11.1681\n",
      "Epoch [1277/2000] - Loss: 8.8544\n",
      "Epoch [1278/2000] - Loss: 10.4759\n",
      "Epoch [1279/2000] - Loss: 9.5689\n",
      "Epoch [1280/2000] - Loss: 14.1059\n",
      "Epoch [1281/2000] - Loss: 10.6193\n",
      "Epoch [1282/2000] - Loss: 13.6700\n",
      "Epoch [1283/2000] - Loss: 14.8534\n",
      "Epoch [1284/2000] - Loss: 16.1957\n",
      "Epoch [1285/2000] - Loss: 10.8798\n",
      "Epoch [1286/2000] - Loss: 14.7192\n",
      "Epoch [1287/2000] - Loss: 9.3582\n",
      "Epoch [1288/2000] - Loss: 12.2258\n",
      "Epoch [1289/2000] - Loss: 6.9109\n",
      "Epoch [1290/2000] - Loss: 15.9211\n",
      "Epoch [1291/2000] - Loss: 11.6471\n",
      "Epoch [1292/2000] - Loss: 12.0507\n",
      "Epoch [1293/2000] - Loss: 22.2029\n",
      "Epoch [1294/2000] - Loss: 11.5912\n",
      "Epoch [1295/2000] - Loss: 9.8593\n",
      "Epoch [1296/2000] - Loss: 28.1047\n",
      "Epoch [1297/2000] - Loss: 5.0456\n",
      "Epoch [1298/2000] - Loss: 18.5515\n",
      "Epoch [1299/2000] - Loss: 9.7632\n",
      "Epoch [1300/2000] - Loss: 14.9199\n",
      "Epoch [1301/2000] - Loss: 8.6476\n",
      "Epoch [1302/2000] - Loss: 12.3246\n",
      "Epoch [1303/2000] - Loss: 16.0555\n",
      "Epoch [1304/2000] - Loss: 6.8841\n",
      "Epoch [1305/2000] - Loss: 7.4746\n",
      "Epoch [1306/2000] - Loss: 9.3778\n",
      "Epoch [1307/2000] - Loss: 23.2315\n",
      "Epoch [1308/2000] - Loss: 9.8375\n",
      "Epoch [1309/2000] - Loss: 9.5398\n",
      "Epoch [1310/2000] - Loss: 10.5820\n",
      "Epoch [1311/2000] - Loss: 9.3556\n",
      "Epoch [1312/2000] - Loss: 8.0150\n",
      "Epoch [1313/2000] - Loss: 21.3929\n",
      "Epoch [1314/2000] - Loss: 4.0377\n",
      "Epoch [1315/2000] - Loss: 15.8999\n",
      "Epoch [1316/2000] - Loss: 4.0824\n",
      "Epoch [1317/2000] - Loss: 8.7681\n",
      "Epoch [1318/2000] - Loss: 15.0446\n",
      "Epoch [1319/2000] - Loss: 10.2862\n",
      "Epoch [1320/2000] - Loss: 22.9175\n",
      "Epoch [1321/2000] - Loss: 8.2719\n",
      "Epoch [1322/2000] - Loss: 12.0698\n",
      "Epoch [1323/2000] - Loss: 13.1403\n",
      "Epoch [1324/2000] - Loss: 10.1292\n",
      "Epoch [1325/2000] - Loss: 12.8064\n",
      "Epoch [1326/2000] - Loss: 8.6691\n",
      "Epoch [1327/2000] - Loss: 8.4094\n",
      "Epoch [1328/2000] - Loss: 19.3592\n",
      "Epoch [1329/2000] - Loss: 13.9715\n",
      "Epoch [1330/2000] - Loss: 26.1450\n",
      "Epoch [1331/2000] - Loss: 8.8509\n",
      "Epoch [1332/2000] - Loss: 22.3637\n",
      "Epoch [1333/2000] - Loss: 6.2738\n",
      "Epoch [1334/2000] - Loss: 24.6608\n",
      "Epoch [1335/2000] - Loss: 17.4334\n",
      "Epoch [1336/2000] - Loss: 7.7692\n",
      "Epoch [1337/2000] - Loss: 18.2379\n",
      "Epoch [1338/2000] - Loss: 15.5666\n",
      "Epoch [1339/2000] - Loss: 15.7470\n",
      "Epoch [1340/2000] - Loss: 8.2733\n",
      "Epoch [1341/2000] - Loss: 12.2871\n",
      "Epoch [1342/2000] - Loss: 10.1788\n",
      "Epoch [1343/2000] - Loss: 10.3511\n",
      "Epoch [1344/2000] - Loss: 17.2311\n",
      "Epoch [1345/2000] - Loss: 5.2996\n",
      "Epoch [1346/2000] - Loss: 11.9654\n",
      "Epoch [1347/2000] - Loss: 10.7032\n",
      "Epoch [1348/2000] - Loss: 18.6862\n",
      "Epoch [1349/2000] - Loss: 11.5956\n",
      "Epoch [1350/2000] - Loss: 11.2808\n",
      "Epoch [1351/2000] - Loss: 17.1017\n",
      "Epoch [1352/2000] - Loss: 6.8212\n",
      "Epoch [1353/2000] - Loss: 18.7916\n",
      "Epoch [1354/2000] - Loss: 17.0250\n",
      "Epoch [1355/2000] - Loss: 10.3346\n",
      "Epoch [1356/2000] - Loss: 17.8136\n",
      "Epoch [1357/2000] - Loss: 8.6751\n",
      "Epoch [1358/2000] - Loss: 7.6450\n",
      "Epoch [1359/2000] - Loss: 10.2413\n",
      "Epoch [1360/2000] - Loss: 11.1777\n",
      "Epoch [1361/2000] - Loss: 13.4104\n",
      "Epoch [1362/2000] - Loss: 7.9542\n",
      "Epoch [1363/2000] - Loss: 9.8376\n",
      "Epoch [1364/2000] - Loss: 15.7874\n",
      "Epoch [1365/2000] - Loss: 9.4790\n",
      "Epoch [1366/2000] - Loss: 9.3987\n",
      "Epoch [1367/2000] - Loss: 10.6599\n",
      "Epoch [1368/2000] - Loss: 15.2367\n",
      "Epoch [1369/2000] - Loss: 9.8948\n",
      "Epoch [1370/2000] - Loss: 12.3710\n",
      "Epoch [1371/2000] - Loss: 16.4687\n",
      "Epoch [1372/2000] - Loss: 21.9525\n",
      "Epoch [1373/2000] - Loss: 14.8869\n",
      "Epoch [1374/2000] - Loss: 11.0092\n",
      "Epoch [1375/2000] - Loss: 7.5274\n",
      "Epoch [1376/2000] - Loss: 19.3233\n",
      "Epoch [1377/2000] - Loss: 14.9800\n",
      "Epoch [1378/2000] - Loss: 13.9938\n",
      "Epoch [1379/2000] - Loss: 11.8730\n",
      "Epoch [1380/2000] - Loss: 15.3891\n",
      "Epoch [1381/2000] - Loss: 7.4442\n",
      "Epoch [1382/2000] - Loss: 10.0929\n",
      "Epoch [1383/2000] - Loss: 7.1452\n",
      "Epoch [1384/2000] - Loss: 15.9376\n",
      "Epoch [1385/2000] - Loss: 15.9346\n",
      "Epoch [1386/2000] - Loss: 17.0575\n",
      "Epoch [1387/2000] - Loss: 7.4426\n",
      "Epoch [1388/2000] - Loss: 9.9864\n",
      "Epoch [1389/2000] - Loss: 10.5914\n",
      "Epoch [1390/2000] - Loss: 26.6793\n",
      "Epoch [1391/2000] - Loss: 9.5031\n",
      "Epoch [1392/2000] - Loss: 15.8417\n",
      "Epoch [1393/2000] - Loss: 24.0671\n",
      "Epoch [1394/2000] - Loss: 11.2427\n",
      "Epoch [1395/2000] - Loss: 14.2166\n",
      "Epoch [1396/2000] - Loss: 8.6286\n",
      "Epoch [1397/2000] - Loss: 8.7999\n",
      "Epoch [1398/2000] - Loss: 8.4956\n",
      "Epoch [1399/2000] - Loss: 12.5018\n",
      "Epoch [1400/2000] - Loss: 10.4787\n",
      "Epoch [1401/2000] - Loss: 13.9636\n",
      "Epoch [1402/2000] - Loss: 9.4241\n",
      "Epoch [1403/2000] - Loss: 5.0481\n",
      "Epoch [1404/2000] - Loss: 13.4402\n",
      "Epoch [1405/2000] - Loss: 26.1189\n",
      "Epoch [1406/2000] - Loss: 13.6750\n",
      "Epoch [1407/2000] - Loss: 28.8195\n",
      "Epoch [1408/2000] - Loss: 13.9527\n",
      "Epoch [1409/2000] - Loss: 12.0571\n",
      "Epoch [1410/2000] - Loss: 10.8132\n",
      "Epoch [1411/2000] - Loss: 11.2255\n",
      "Epoch [1412/2000] - Loss: 8.8545\n",
      "Epoch [1413/2000] - Loss: 11.1883\n",
      "Epoch [1414/2000] - Loss: 19.8015\n",
      "Epoch [1415/2000] - Loss: 10.9067\n",
      "Epoch [1416/2000] - Loss: 11.0021\n",
      "Epoch [1417/2000] - Loss: 11.4952\n",
      "Epoch [1418/2000] - Loss: 15.0589\n",
      "Epoch [1419/2000] - Loss: 19.5833\n",
      "Epoch [1420/2000] - Loss: 5.9218\n",
      "Epoch [1421/2000] - Loss: 7.4829\n",
      "Epoch [1422/2000] - Loss: 12.4876\n",
      "Epoch [1423/2000] - Loss: 17.3564\n",
      "Epoch [1424/2000] - Loss: 10.6569\n",
      "Epoch [1425/2000] - Loss: 10.8906\n",
      "Epoch [1426/2000] - Loss: 7.5675\n",
      "Epoch [1427/2000] - Loss: 8.0167\n",
      "Epoch [1428/2000] - Loss: 11.9136\n",
      "Epoch [1429/2000] - Loss: 14.5430\n",
      "Epoch [1430/2000] - Loss: 9.9443\n",
      "Epoch [1431/2000] - Loss: 11.6959\n",
      "Epoch [1432/2000] - Loss: 11.0674\n",
      "Epoch [1433/2000] - Loss: 5.7890\n",
      "Epoch [1434/2000] - Loss: 11.4344\n",
      "Epoch [1435/2000] - Loss: 9.3106\n",
      "Epoch [1436/2000] - Loss: 6.4923\n",
      "Epoch [1437/2000] - Loss: 14.4746\n",
      "Epoch [1438/2000] - Loss: 9.4737\n",
      "Epoch [1439/2000] - Loss: 15.9842\n",
      "Epoch [1440/2000] - Loss: 8.4941\n",
      "Epoch [1441/2000] - Loss: 11.0895\n",
      "Epoch [1442/2000] - Loss: 14.1651\n",
      "Epoch [1443/2000] - Loss: 9.1558\n",
      "Epoch [1444/2000] - Loss: 15.9270\n",
      "Epoch [1445/2000] - Loss: 12.6140\n",
      "Epoch [1446/2000] - Loss: 10.2007\n",
      "Epoch [1447/2000] - Loss: 14.7421\n",
      "Epoch [1448/2000] - Loss: 20.9122\n",
      "Epoch [1449/2000] - Loss: 18.0412\n",
      "Epoch [1450/2000] - Loss: 15.8995\n",
      "Epoch [1451/2000] - Loss: 13.9909\n",
      "Epoch [1452/2000] - Loss: 10.8348\n",
      "Epoch [1453/2000] - Loss: 19.6139\n",
      "Epoch [1454/2000] - Loss: 11.4642\n",
      "Epoch [1455/2000] - Loss: 15.3986\n",
      "Epoch [1456/2000] - Loss: 16.7454\n",
      "Epoch [1457/2000] - Loss: 16.4575\n",
      "Epoch [1458/2000] - Loss: 11.5775\n",
      "Epoch [1459/2000] - Loss: 9.7206\n",
      "Epoch [1460/2000] - Loss: 9.8288\n",
      "Epoch [1461/2000] - Loss: 14.9518\n",
      "Epoch [1462/2000] - Loss: 16.4346\n",
      "Epoch [1463/2000] - Loss: 4.0282\n",
      "Epoch [1464/2000] - Loss: 22.6293\n",
      "Epoch [1465/2000] - Loss: 6.9665\n",
      "Epoch [1466/2000] - Loss: 7.8548\n",
      "Epoch [1467/2000] - Loss: 12.7093\n",
      "Epoch [1468/2000] - Loss: 4.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1469/2000] - Loss: 21.9164\n",
      "Epoch [1470/2000] - Loss: 9.6855\n",
      "Epoch [1471/2000] - Loss: 12.1008\n",
      "Epoch [1472/2000] - Loss: 6.2301\n",
      "Epoch [1473/2000] - Loss: 13.7042\n",
      "Epoch [1474/2000] - Loss: 16.7507\n",
      "Epoch [1475/2000] - Loss: 23.0378\n",
      "Epoch [1476/2000] - Loss: 11.8033\n",
      "Epoch [1477/2000] - Loss: 13.1677\n",
      "Epoch [1478/2000] - Loss: 8.2684\n",
      "Epoch [1479/2000] - Loss: 13.1013\n",
      "Epoch [1480/2000] - Loss: 12.8199\n",
      "Epoch [1481/2000] - Loss: 12.9140\n",
      "Epoch [1482/2000] - Loss: 9.8867\n",
      "Epoch [1483/2000] - Loss: 16.0153\n",
      "Epoch [1484/2000] - Loss: 10.3566\n",
      "Epoch [1485/2000] - Loss: 6.7386\n",
      "Epoch [1486/2000] - Loss: 18.1375\n",
      "Epoch [1487/2000] - Loss: 10.9039\n",
      "Epoch [1488/2000] - Loss: 10.2991\n",
      "Epoch [1489/2000] - Loss: 8.1722\n",
      "Epoch [1490/2000] - Loss: 16.8222\n",
      "Epoch [1491/2000] - Loss: 10.3694\n",
      "Epoch [1492/2000] - Loss: 15.3442\n",
      "Epoch [1493/2000] - Loss: 12.0899\n",
      "Epoch [1494/2000] - Loss: 4.0117\n",
      "Epoch [1495/2000] - Loss: 14.1113\n",
      "Epoch [1496/2000] - Loss: 6.5852\n",
      "Epoch [1497/2000] - Loss: 21.7629\n",
      "Epoch [1498/2000] - Loss: 15.0831\n",
      "Epoch [1499/2000] - Loss: 10.9681\n",
      "Epoch [1500/2000] - Loss: 23.0323\n",
      "Epoch [1501/2000] - Loss: 12.8761\n",
      "Epoch [1502/2000] - Loss: 14.8382\n",
      "Epoch [1503/2000] - Loss: 9.2768\n",
      "Epoch [1504/2000] - Loss: 7.9920\n",
      "Epoch [1505/2000] - Loss: 22.4770\n",
      "Epoch [1506/2000] - Loss: 10.9384\n",
      "Epoch [1507/2000] - Loss: 12.4970\n",
      "Epoch [1508/2000] - Loss: 11.3510\n",
      "Epoch [1509/2000] - Loss: 12.3231\n",
      "Epoch [1510/2000] - Loss: 11.0769\n",
      "Epoch [1511/2000] - Loss: 10.9905\n",
      "Epoch [1512/2000] - Loss: 13.1742\n",
      "Epoch [1513/2000] - Loss: 12.0734\n",
      "Epoch [1514/2000] - Loss: 6.6275\n",
      "Epoch [1515/2000] - Loss: 23.0442\n",
      "Epoch [1516/2000] - Loss: 6.2911\n",
      "Epoch [1517/2000] - Loss: 8.9439\n",
      "Epoch [1518/2000] - Loss: 23.2664\n",
      "Epoch [1519/2000] - Loss: 8.0562\n",
      "Epoch [1520/2000] - Loss: 6.0906\n",
      "Epoch [1521/2000] - Loss: 15.2483\n",
      "Epoch [1522/2000] - Loss: 9.1777\n",
      "Epoch [1523/2000] - Loss: 17.6159\n",
      "Epoch [1524/2000] - Loss: 9.3884\n",
      "Epoch [1525/2000] - Loss: 15.1952\n",
      "Epoch [1526/2000] - Loss: 12.5486\n",
      "Epoch [1527/2000] - Loss: 6.3737\n",
      "Epoch [1528/2000] - Loss: 14.0673\n",
      "Epoch [1529/2000] - Loss: 12.4922\n",
      "Epoch [1530/2000] - Loss: 11.6697\n",
      "Epoch [1531/2000] - Loss: 16.2738\n",
      "Epoch [1532/2000] - Loss: 18.6721\n",
      "Epoch [1533/2000] - Loss: 10.5194\n",
      "Epoch [1534/2000] - Loss: 12.3615\n",
      "Epoch [1535/2000] - Loss: 10.5723\n",
      "Epoch [1536/2000] - Loss: 7.9056\n",
      "Epoch [1537/2000] - Loss: 11.5613\n",
      "Epoch [1538/2000] - Loss: 15.4475\n",
      "Epoch [1539/2000] - Loss: 10.2412\n",
      "Epoch [1540/2000] - Loss: 9.2762\n",
      "Epoch [1541/2000] - Loss: 9.3186\n",
      "Epoch [1542/2000] - Loss: 16.5317\n",
      "Epoch [1543/2000] - Loss: 7.2522\n",
      "Epoch [1544/2000] - Loss: 11.6126\n",
      "Epoch [1545/2000] - Loss: 17.2173\n",
      "Epoch [1546/2000] - Loss: 18.0018\n",
      "Epoch [1547/2000] - Loss: 14.7110\n",
      "Epoch [1548/2000] - Loss: 7.5709\n",
      "Epoch [1549/2000] - Loss: 10.4782\n",
      "Epoch [1550/2000] - Loss: 13.4774\n",
      "Epoch [1551/2000] - Loss: 14.6066\n",
      "Epoch [1552/2000] - Loss: 25.3384\n",
      "Epoch [1553/2000] - Loss: 13.4367\n",
      "Epoch [1554/2000] - Loss: 16.3399\n",
      "Epoch [1555/2000] - Loss: 8.6751\n",
      "Epoch [1556/2000] - Loss: 17.4121\n",
      "Epoch [1557/2000] - Loss: 7.0119\n",
      "Epoch [1558/2000] - Loss: 10.4281\n",
      "Epoch [1559/2000] - Loss: 7.0433\n",
      "Epoch [1560/2000] - Loss: 11.9859\n",
      "Epoch [1561/2000] - Loss: 11.6484\n",
      "Epoch [1562/2000] - Loss: 12.8370\n",
      "Epoch [1563/2000] - Loss: 8.6774\n",
      "Epoch [1564/2000] - Loss: 26.6352\n",
      "Epoch [1565/2000] - Loss: 20.6246\n",
      "Epoch [1566/2000] - Loss: 15.3561\n",
      "Epoch [1567/2000] - Loss: 14.6063\n",
      "Epoch [1568/2000] - Loss: 14.1355\n",
      "Epoch [1569/2000] - Loss: 8.8435\n",
      "Epoch [1570/2000] - Loss: 15.7715\n",
      "Epoch [1571/2000] - Loss: 5.9796\n",
      "Epoch [1572/2000] - Loss: 7.4343\n",
      "Epoch [1573/2000] - Loss: 10.6835\n",
      "Epoch [1574/2000] - Loss: 13.6523\n",
      "Epoch [1575/2000] - Loss: 9.2536\n",
      "Epoch [1576/2000] - Loss: 6.6535\n",
      "Epoch [1577/2000] - Loss: 6.3635\n",
      "Epoch [1578/2000] - Loss: 11.3676\n",
      "Epoch [1579/2000] - Loss: 20.2016\n",
      "Epoch [1580/2000] - Loss: 20.5221\n",
      "Epoch [1581/2000] - Loss: 9.9822\n",
      "Epoch [1582/2000] - Loss: 13.2623\n",
      "Epoch [1583/2000] - Loss: 7.8061\n",
      "Epoch [1584/2000] - Loss: 12.0820\n",
      "Epoch [1585/2000] - Loss: 10.3033\n",
      "Epoch [1586/2000] - Loss: 9.3751\n",
      "Epoch [1587/2000] - Loss: 8.3384\n",
      "Epoch [1588/2000] - Loss: 12.5820\n",
      "Epoch [1589/2000] - Loss: 9.9505\n",
      "Epoch [1590/2000] - Loss: 23.2624\n",
      "Epoch [1591/2000] - Loss: 14.9891\n",
      "Epoch [1592/2000] - Loss: 9.3826\n",
      "Epoch [1593/2000] - Loss: 6.4728\n",
      "Epoch [1594/2000] - Loss: 25.5417\n",
      "Epoch [1595/2000] - Loss: 18.8063\n",
      "Epoch [1596/2000] - Loss: 14.0987\n",
      "Epoch [1597/2000] - Loss: 18.5424\n",
      "Epoch [1598/2000] - Loss: 8.1754\n",
      "Epoch [1599/2000] - Loss: 7.6365\n",
      "Epoch [1600/2000] - Loss: 8.2169\n",
      "Epoch [1601/2000] - Loss: 10.7285\n",
      "Epoch [1602/2000] - Loss: 23.3361\n",
      "Epoch [1603/2000] - Loss: 5.2349\n",
      "Epoch [1604/2000] - Loss: 9.5932\n",
      "Epoch [1605/2000] - Loss: 13.1914\n",
      "Epoch [1606/2000] - Loss: 15.6681\n",
      "Epoch [1607/2000] - Loss: 9.2714\n",
      "Epoch [1608/2000] - Loss: 10.5494\n",
      "Epoch [1609/2000] - Loss: 11.6239\n",
      "Epoch [1610/2000] - Loss: 10.6011\n",
      "Epoch [1611/2000] - Loss: 10.8218\n",
      "Epoch [1612/2000] - Loss: 6.5256\n",
      "Epoch [1613/2000] - Loss: 15.1734\n",
      "Epoch [1614/2000] - Loss: 14.3636\n",
      "Epoch [1615/2000] - Loss: 10.4929\n",
      "Epoch [1616/2000] - Loss: 17.0565\n",
      "Epoch [1617/2000] - Loss: 9.3524\n",
      "Epoch [1618/2000] - Loss: 14.1499\n",
      "Epoch [1619/2000] - Loss: 18.2458\n",
      "Epoch [1620/2000] - Loss: 16.8940\n",
      "Epoch [1621/2000] - Loss: 8.2781\n",
      "Epoch [1622/2000] - Loss: 14.8134\n",
      "Epoch [1623/2000] - Loss: 19.2350\n",
      "Epoch [1624/2000] - Loss: 8.2411\n",
      "Epoch [1625/2000] - Loss: 14.5954\n",
      "Epoch [1626/2000] - Loss: 11.7617\n",
      "Epoch [1627/2000] - Loss: 11.5031\n",
      "Epoch [1628/2000] - Loss: 16.0858\n",
      "Epoch [1629/2000] - Loss: 8.5434\n",
      "Epoch [1630/2000] - Loss: 12.3266\n",
      "Epoch [1631/2000] - Loss: 9.0696\n",
      "Epoch [1632/2000] - Loss: 10.6715\n",
      "Epoch [1633/2000] - Loss: 8.7403\n",
      "Epoch [1634/2000] - Loss: 13.0340\n",
      "Epoch [1635/2000] - Loss: 15.9448\n",
      "Epoch [1636/2000] - Loss: 17.9785\n",
      "Epoch [1637/2000] - Loss: 11.3948\n",
      "Epoch [1638/2000] - Loss: 11.0093\n",
      "Epoch [1639/2000] - Loss: 12.9414\n",
      "Epoch [1640/2000] - Loss: 14.1243\n",
      "Epoch [1641/2000] - Loss: 12.9896\n",
      "Epoch [1642/2000] - Loss: 12.3507\n",
      "Epoch [1643/2000] - Loss: 7.2652\n",
      "Epoch [1644/2000] - Loss: 9.6804\n",
      "Epoch [1645/2000] - Loss: 20.5729\n",
      "Epoch [1646/2000] - Loss: 10.5120\n",
      "Epoch [1647/2000] - Loss: 17.7943\n",
      "Epoch [1648/2000] - Loss: 5.4430\n",
      "Epoch [1649/2000] - Loss: 9.1260\n",
      "Epoch [1650/2000] - Loss: 11.4302\n",
      "Epoch [1651/2000] - Loss: 17.3345\n",
      "Epoch [1652/2000] - Loss: 8.8237\n",
      "Epoch [1653/2000] - Loss: 14.2631\n",
      "Epoch [1654/2000] - Loss: 10.3948\n",
      "Epoch [1655/2000] - Loss: 13.5620\n",
      "Epoch [1656/2000] - Loss: 6.7831\n",
      "Epoch [1657/2000] - Loss: 16.5963\n",
      "Epoch [1658/2000] - Loss: 10.2345\n",
      "Epoch [1659/2000] - Loss: 7.9145\n",
      "Epoch [1660/2000] - Loss: 19.3330\n",
      "Epoch [1661/2000] - Loss: 8.5633\n",
      "Epoch [1662/2000] - Loss: 9.6798\n",
      "Epoch [1663/2000] - Loss: 8.5590\n",
      "Epoch [1664/2000] - Loss: 8.2206\n",
      "Epoch [1665/2000] - Loss: 13.5833\n",
      "Epoch [1666/2000] - Loss: 17.1443\n",
      "Epoch [1667/2000] - Loss: 9.5647\n",
      "Epoch [1668/2000] - Loss: 14.8949\n",
      "Epoch [1669/2000] - Loss: 13.9423\n",
      "Epoch [1670/2000] - Loss: 19.5575\n",
      "Epoch [1671/2000] - Loss: 12.9778\n",
      "Epoch [1672/2000] - Loss: 13.1192\n",
      "Epoch [1673/2000] - Loss: 8.7173\n",
      "Epoch [1674/2000] - Loss: 12.6152\n",
      "Epoch [1675/2000] - Loss: 12.9252\n",
      "Epoch [1676/2000] - Loss: 7.6235\n",
      "Epoch [1677/2000] - Loss: 8.8084\n",
      "Epoch [1678/2000] - Loss: 9.7864\n",
      "Epoch [1679/2000] - Loss: 18.2735\n",
      "Epoch [1680/2000] - Loss: 25.0685\n",
      "Epoch [1681/2000] - Loss: 24.3869\n",
      "Epoch [1682/2000] - Loss: 9.6667\n",
      "Epoch [1683/2000] - Loss: 8.9623\n",
      "Epoch [1684/2000] - Loss: 20.8139\n",
      "Epoch [1685/2000] - Loss: 10.7671\n",
      "Epoch [1686/2000] - Loss: 9.3696\n",
      "Epoch [1687/2000] - Loss: 12.7468\n",
      "Epoch [1688/2000] - Loss: 13.4937\n",
      "Epoch [1689/2000] - Loss: 5.6882\n",
      "Epoch [1690/2000] - Loss: 15.5911\n",
      "Epoch [1691/2000] - Loss: 15.6068\n",
      "Epoch [1692/2000] - Loss: 16.4444\n",
      "Epoch [1693/2000] - Loss: 13.7582\n",
      "Epoch [1694/2000] - Loss: 8.7830\n",
      "Epoch [1695/2000] - Loss: 16.9466\n",
      "Epoch [1696/2000] - Loss: 16.1843\n",
      "Epoch [1697/2000] - Loss: 13.1196\n",
      "Epoch [1698/2000] - Loss: 13.1218\n",
      "Epoch [1699/2000] - Loss: 13.2481\n",
      "Epoch [1700/2000] - Loss: 5.7528\n",
      "Epoch [1701/2000] - Loss: 12.8870\n",
      "Epoch [1702/2000] - Loss: 11.5268\n",
      "Epoch [1703/2000] - Loss: 13.0592\n",
      "Epoch [1704/2000] - Loss: 23.8644\n",
      "Epoch [1705/2000] - Loss: 12.2276\n",
      "Epoch [1706/2000] - Loss: 13.9398\n",
      "Epoch [1707/2000] - Loss: 21.7974\n",
      "Epoch [1708/2000] - Loss: 17.0363\n",
      "Epoch [1709/2000] - Loss: 10.5903\n",
      "Epoch [1710/2000] - Loss: 9.8947\n",
      "Epoch [1711/2000] - Loss: 9.1978\n",
      "Epoch [1712/2000] - Loss: 10.9162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1713/2000] - Loss: 9.4572\n",
      "Epoch [1714/2000] - Loss: 17.9286\n",
      "Epoch [1715/2000] - Loss: 10.8003\n",
      "Epoch [1716/2000] - Loss: 8.4692\n",
      "Epoch [1717/2000] - Loss: 12.7139\n",
      "Epoch [1718/2000] - Loss: 18.3066\n",
      "Epoch [1719/2000] - Loss: 11.7509\n",
      "Epoch [1720/2000] - Loss: 12.3635\n",
      "Epoch [1721/2000] - Loss: 14.1722\n",
      "Epoch [1722/2000] - Loss: 6.7340\n",
      "Epoch [1723/2000] - Loss: 9.7884\n",
      "Epoch [1724/2000] - Loss: 7.6497\n",
      "Epoch [1725/2000] - Loss: 11.6019\n",
      "Epoch [1726/2000] - Loss: 10.3473\n",
      "Epoch [1727/2000] - Loss: 8.9465\n",
      "Epoch [1728/2000] - Loss: 15.5096\n",
      "Epoch [1729/2000] - Loss: 9.9644\n",
      "Epoch [1730/2000] - Loss: 12.4620\n",
      "Epoch [1731/2000] - Loss: 13.2724\n",
      "Epoch [1732/2000] - Loss: 11.8421\n",
      "Epoch [1733/2000] - Loss: 13.8164\n",
      "Epoch [1734/2000] - Loss: 21.7927\n",
      "Epoch [1735/2000] - Loss: 7.9797\n",
      "Epoch [1736/2000] - Loss: 16.7074\n",
      "Epoch [1737/2000] - Loss: 13.3548\n",
      "Epoch [1738/2000] - Loss: 18.1760\n",
      "Epoch [1739/2000] - Loss: 21.5213\n",
      "Epoch [1740/2000] - Loss: 10.0617\n",
      "Epoch [1741/2000] - Loss: 12.1873\n",
      "Epoch [1742/2000] - Loss: 14.5176\n",
      "Epoch [1743/2000] - Loss: 10.4681\n",
      "Epoch [1744/2000] - Loss: 14.3355\n",
      "Epoch [1745/2000] - Loss: 18.9383\n",
      "Epoch [1746/2000] - Loss: 8.7255\n",
      "Epoch [1747/2000] - Loss: 10.0708\n",
      "Epoch [1748/2000] - Loss: 10.3273\n",
      "Epoch [1749/2000] - Loss: 16.0944\n",
      "Epoch [1750/2000] - Loss: 8.9120\n",
      "Epoch [1751/2000] - Loss: 20.7570\n",
      "Epoch [1752/2000] - Loss: 11.0916\n",
      "Epoch [1753/2000] - Loss: 7.2836\n",
      "Epoch [1754/2000] - Loss: 10.7870\n",
      "Epoch [1755/2000] - Loss: 5.9545\n",
      "Epoch [1756/2000] - Loss: 7.9356\n",
      "Epoch [1757/2000] - Loss: 5.6580\n",
      "Epoch [1758/2000] - Loss: 7.7968\n",
      "Epoch [1759/2000] - Loss: 12.4765\n",
      "Epoch [1760/2000] - Loss: 12.8161\n",
      "Epoch [1761/2000] - Loss: 12.9242\n",
      "Epoch [1762/2000] - Loss: 9.2160\n",
      "Epoch [1763/2000] - Loss: 10.0393\n",
      "Epoch [1764/2000] - Loss: 17.3742\n",
      "Epoch [1765/2000] - Loss: 34.9418\n",
      "Epoch [1766/2000] - Loss: 12.2320\n",
      "Epoch [1767/2000] - Loss: 4.1814\n",
      "Epoch [1768/2000] - Loss: 26.7681\n",
      "Epoch [1769/2000] - Loss: 16.2358\n",
      "Epoch [1770/2000] - Loss: 16.5978\n",
      "Epoch [1771/2000] - Loss: 13.2415\n",
      "Epoch [1772/2000] - Loss: 9.4757\n",
      "Epoch [1773/2000] - Loss: 23.1363\n",
      "Epoch [1774/2000] - Loss: 10.1918\n",
      "Epoch [1775/2000] - Loss: 12.6437\n",
      "Epoch [1776/2000] - Loss: 17.5368\n",
      "Epoch [1777/2000] - Loss: 12.3891\n",
      "Epoch [1778/2000] - Loss: 21.9682\n",
      "Epoch [1779/2000] - Loss: 9.2026\n",
      "Epoch [1780/2000] - Loss: 13.5806\n",
      "Epoch [1781/2000] - Loss: 7.6174\n",
      "Epoch [1782/2000] - Loss: 8.6903\n",
      "Epoch [1783/2000] - Loss: 4.9219\n",
      "Epoch [1784/2000] - Loss: 11.1179\n",
      "Epoch [1785/2000] - Loss: 9.7206\n",
      "Epoch [1786/2000] - Loss: 15.0144\n",
      "Epoch [1787/2000] - Loss: 24.3271\n",
      "Epoch [1788/2000] - Loss: 14.0931\n",
      "Epoch [1789/2000] - Loss: 6.9465\n",
      "Epoch [1790/2000] - Loss: 13.9715\n",
      "Epoch [1791/2000] - Loss: 9.8685\n",
      "Epoch [1792/2000] - Loss: 10.9420\n",
      "Epoch [1793/2000] - Loss: 12.3353\n",
      "Epoch [1794/2000] - Loss: 23.4613\n",
      "Epoch [1795/2000] - Loss: 17.0017\n",
      "Epoch [1796/2000] - Loss: 6.6453\n",
      "Epoch [1797/2000] - Loss: 6.6692\n",
      "Epoch [1798/2000] - Loss: 12.1945\n",
      "Epoch [1799/2000] - Loss: 8.0038\n",
      "Epoch [1800/2000] - Loss: 29.0238\n",
      "Epoch [1801/2000] - Loss: 7.7513\n",
      "Epoch [1802/2000] - Loss: 5.8499\n",
      "Epoch [1803/2000] - Loss: 6.8757\n",
      "Epoch [1804/2000] - Loss: 10.3890\n",
      "Epoch [1805/2000] - Loss: 14.4925\n",
      "Epoch [1806/2000] - Loss: 15.7746\n",
      "Epoch [1807/2000] - Loss: 9.0282\n",
      "Epoch [1808/2000] - Loss: 6.5398\n",
      "Epoch [1809/2000] - Loss: 21.6420\n",
      "Epoch [1810/2000] - Loss: 7.7901\n",
      "Epoch [1811/2000] - Loss: 11.4530\n",
      "Epoch [1812/2000] - Loss: 19.5859\n",
      "Epoch [1813/2000] - Loss: 10.8583\n",
      "Epoch [1814/2000] - Loss: 7.0487\n",
      "Epoch [1815/2000] - Loss: 7.0633\n",
      "Epoch [1816/2000] - Loss: 12.0956\n",
      "Epoch [1817/2000] - Loss: 15.5367\n",
      "Epoch [1818/2000] - Loss: 17.8078\n",
      "Epoch [1819/2000] - Loss: 5.3958\n",
      "Epoch [1820/2000] - Loss: 12.2600\n",
      "Epoch [1821/2000] - Loss: 5.3886\n",
      "Epoch [1822/2000] - Loss: 10.8369\n",
      "Epoch [1823/2000] - Loss: 11.8919\n",
      "Epoch [1824/2000] - Loss: 11.3090\n",
      "Epoch [1825/2000] - Loss: 12.6507\n",
      "Epoch [1826/2000] - Loss: 21.2447\n",
      "Epoch [1827/2000] - Loss: 16.4263\n",
      "Epoch [1828/2000] - Loss: 5.0134\n",
      "Epoch [1829/2000] - Loss: 17.1369\n",
      "Epoch [1830/2000] - Loss: 18.9761\n",
      "Epoch [1831/2000] - Loss: 11.8958\n",
      "Epoch [1832/2000] - Loss: 9.4100\n",
      "Epoch [1833/2000] - Loss: 9.0084\n",
      "Epoch [1834/2000] - Loss: 9.0236\n",
      "Epoch [1835/2000] - Loss: 18.3439\n",
      "Epoch [1836/2000] - Loss: 5.9456\n",
      "Epoch [1837/2000] - Loss: 6.1070\n",
      "Epoch [1838/2000] - Loss: 9.5821\n",
      "Epoch [1839/2000] - Loss: 16.3318\n",
      "Epoch [1840/2000] - Loss: 13.8776\n",
      "Epoch [1841/2000] - Loss: 20.6200\n",
      "Epoch [1842/2000] - Loss: 7.6078\n",
      "Epoch [1843/2000] - Loss: 14.7264\n",
      "Epoch [1844/2000] - Loss: 10.9887\n",
      "Epoch [1845/2000] - Loss: 22.0503\n",
      "Epoch [1846/2000] - Loss: 8.0449\n",
      "Epoch [1847/2000] - Loss: 9.9130\n",
      "Epoch [1848/2000] - Loss: 17.3686\n",
      "Epoch [1849/2000] - Loss: 9.3787\n",
      "Epoch [1850/2000] - Loss: 9.9358\n",
      "Epoch [1851/2000] - Loss: 11.9083\n",
      "Epoch [1852/2000] - Loss: 13.2898\n",
      "Epoch [1853/2000] - Loss: 10.0330\n",
      "Epoch [1854/2000] - Loss: 12.9679\n",
      "Epoch [1855/2000] - Loss: 8.1627\n",
      "Epoch [1856/2000] - Loss: 10.7815\n",
      "Epoch [1857/2000] - Loss: 15.2657\n",
      "Epoch [1858/2000] - Loss: 15.6243\n",
      "Epoch [1859/2000] - Loss: 9.7631\n",
      "Epoch [1860/2000] - Loss: 17.0606\n",
      "Epoch [1861/2000] - Loss: 10.6799\n",
      "Epoch [1862/2000] - Loss: 14.2543\n",
      "Epoch [1863/2000] - Loss: 15.0759\n",
      "Epoch [1864/2000] - Loss: 7.4951\n",
      "Epoch [1865/2000] - Loss: 6.2985\n",
      "Epoch [1866/2000] - Loss: 7.0899\n",
      "Epoch [1867/2000] - Loss: 10.7435\n",
      "Epoch [1868/2000] - Loss: 13.5197\n",
      "Epoch [1869/2000] - Loss: 20.0408\n",
      "Epoch [1870/2000] - Loss: 10.1896\n",
      "Epoch [1871/2000] - Loss: 16.1067\n",
      "Epoch [1872/2000] - Loss: 20.1023\n",
      "Epoch [1873/2000] - Loss: 13.7255\n",
      "Epoch [1874/2000] - Loss: 13.8854\n",
      "Epoch [1875/2000] - Loss: 13.3224\n",
      "Epoch [1876/2000] - Loss: 12.5183\n",
      "Epoch [1877/2000] - Loss: 9.0411\n",
      "Epoch [1878/2000] - Loss: 14.0254\n",
      "Epoch [1879/2000] - Loss: 10.4549\n",
      "Epoch [1880/2000] - Loss: 13.3083\n",
      "Epoch [1881/2000] - Loss: 18.0378\n",
      "Epoch [1882/2000] - Loss: 14.6250\n",
      "Epoch [1883/2000] - Loss: 22.5404\n",
      "Epoch [1884/2000] - Loss: 13.8032\n",
      "Epoch [1885/2000] - Loss: 17.4582\n",
      "Epoch [1886/2000] - Loss: 20.6282\n",
      "Epoch [1887/2000] - Loss: 15.0758\n",
      "Epoch [1888/2000] - Loss: 14.2247\n",
      "Epoch [1889/2000] - Loss: 10.7830\n",
      "Epoch [1890/2000] - Loss: 15.4426\n",
      "Epoch [1891/2000] - Loss: 10.5932\n",
      "Epoch [1892/2000] - Loss: 12.7503\n",
      "Epoch [1893/2000] - Loss: 7.6095\n",
      "Epoch [1894/2000] - Loss: 23.8865\n",
      "Epoch [1895/2000] - Loss: 13.8894\n",
      "Epoch [1896/2000] - Loss: 13.2679\n",
      "Epoch [1897/2000] - Loss: 15.7007\n",
      "Epoch [1898/2000] - Loss: 5.4486\n",
      "Epoch [1899/2000] - Loss: 7.5088\n",
      "Epoch [1900/2000] - Loss: 10.0052\n",
      "Epoch [1901/2000] - Loss: 10.6839\n",
      "Epoch [1902/2000] - Loss: 10.2395\n",
      "Epoch [1903/2000] - Loss: 8.6956\n",
      "Epoch [1904/2000] - Loss: 7.8501\n",
      "Epoch [1905/2000] - Loss: 8.0551\n",
      "Epoch [1906/2000] - Loss: 18.0752\n",
      "Epoch [1907/2000] - Loss: 11.0644\n",
      "Epoch [1908/2000] - Loss: 10.3789\n",
      "Epoch [1909/2000] - Loss: 10.5616\n",
      "Epoch [1910/2000] - Loss: 9.1876\n",
      "Epoch [1911/2000] - Loss: 18.7095\n",
      "Epoch [1912/2000] - Loss: 10.6685\n",
      "Epoch [1913/2000] - Loss: 15.1915\n",
      "Epoch [1914/2000] - Loss: 6.9358\n",
      "Epoch [1915/2000] - Loss: 6.8027\n",
      "Epoch [1916/2000] - Loss: 10.1296\n",
      "Epoch [1917/2000] - Loss: 14.3698\n",
      "Epoch [1918/2000] - Loss: 19.7033\n",
      "Epoch [1919/2000] - Loss: 13.7502\n",
      "Epoch [1920/2000] - Loss: 13.2226\n",
      "Epoch [1921/2000] - Loss: 15.1373\n",
      "Epoch [1922/2000] - Loss: 14.6301\n",
      "Epoch [1923/2000] - Loss: 22.0739\n",
      "Epoch [1924/2000] - Loss: 15.3814\n",
      "Epoch [1925/2000] - Loss: 12.3642\n",
      "Epoch [1926/2000] - Loss: 6.8267\n",
      "Epoch [1927/2000] - Loss: 8.4033\n",
      "Epoch [1928/2000] - Loss: 10.3078\n",
      "Epoch [1929/2000] - Loss: 11.2789\n",
      "Epoch [1930/2000] - Loss: 10.6757\n",
      "Epoch [1931/2000] - Loss: 11.2791\n",
      "Epoch [1932/2000] - Loss: 12.0631\n",
      "Epoch [1933/2000] - Loss: 11.2732\n",
      "Epoch [1934/2000] - Loss: 29.0951\n",
      "Epoch [1935/2000] - Loss: 9.6246\n",
      "Epoch [1936/2000] - Loss: 10.8698\n",
      "Epoch [1937/2000] - Loss: 5.0062\n",
      "Epoch [1938/2000] - Loss: 7.5265\n"
     ]
    }
   ],
   "source": [
    "model = FCED()\n",
    "best_model_mul_FCED = train_model(model, 'best_model_mul_FCED.pth', train_dataloader, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c44be85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
